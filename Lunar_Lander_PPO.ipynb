{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A8XhduOFrRAL"
   },
   "source": [
    "# Lunar Lander - PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YpECY5HXrmu6",
    "outputId": "373ad9f7-8d64-47c9-8abb-593ff0988ecd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: swig in c:\\users\\viswa\\anaconda3\\lib\\site-packages (4.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium[box2d] in c:\\users\\viswa\\anaconda3\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\viswa\\anaconda3\\lib\\site-packages (from gymnasium[box2d]) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\viswa\\anaconda3\\lib\\site-packages (from gymnasium[box2d]) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\viswa\\anaconda3\\lib\\site-packages (from gymnasium[box2d]) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\viswa\\anaconda3\\lib\\site-packages (from gymnasium[box2d]) (0.0.4)\n",
      "Requirement already satisfied: box2d-py==2.3.5 in c:\\users\\viswa\\anaconda3\\lib\\site-packages (from gymnasium[box2d]) (2.3.5)\n",
      "Requirement already satisfied: pygame>=2.1.3 in c:\\users\\viswa\\anaconda3\\lib\\site-packages (from gymnasium[box2d]) (2.6.1)\n",
      "Requirement already satisfied: swig==4.* in c:\\users\\viswa\\anaconda3\\lib\\site-packages (from gymnasium[box2d]) (4.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from abc import ABC, abstractmethod\n",
    "from enum import Enum\n",
    "from gymnasium import spaces\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "%pip install swig\n",
    "%pip install \"gymnasium[box2d]\"\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9dvSWo0gprgX"
   },
   "source": [
    "## Agent-related Class Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "G3doDY6OpiYV"
   },
   "outputs": [],
   "source": [
    "class PPONet(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(PPONet, self).__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.actor = nn.Linear(128, action_dim)\n",
    "        self.critic = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.shared(x)\n",
    "        logits = self.actor(x)\n",
    "        value = self.critic(x)\n",
    "        return logits, value\n",
    "\n",
    "class PPOAgent:\n",
    "    BATCH_SIZE = 32\n",
    "    CLIP_PARAM = 0.2\n",
    "    GAMMA = 0.99\n",
    "    LR = 1e-4\n",
    "    GAE_LAMBDA = 0.95\n",
    "    PPO_EPOCHS = 10\n",
    "    VALUE_LOSS_COEF = 0.5\n",
    "    ENTROPY_COEF = 0.01\n",
    "\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.policy_net = PPONet(state_dim, action_dim).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.LR)\n",
    "        self.memory = []\n",
    "        self.clip = self.CLIP_PARAM\n",
    "\n",
    "    def store_transition(self, state, action, reward, next_state, done, log_prob, value):\n",
    "        self.memory.append((state, action, reward, next_state, done, log_prob, value))\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "        logits, value = self.policy_net(state_tensor)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        action = torch.multinomial(probs, 1).item()\n",
    "        log_prob = log_probs[0, action].item()\n",
    "        \n",
    "        return action, log_prob, value.item()\n",
    "\n",
    "    def compute_gae(self, rewards, values, next_value, dones):\n",
    "        advantages = []\n",
    "        gae = 0\n",
    "        values = values + [next_value]\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            delta = rewards[t] + self.GAMMA * values[t + 1] * (1 - dones[t]) - values[t]\n",
    "            gae = delta + self.GAMMA * self.GAE_LAMBDA * (1 - dones[t]) * gae\n",
    "            advantages.insert(0, gae)\n",
    "        advantages = torch.tensor(advantages, dtype=torch.float32, device=self.device)\n",
    "        returns = advantages + torch.tensor(values[:-1], dtype=torch.float32, device=self.device)\n",
    "        \n",
    "        return advantages, returns\n",
    "\n",
    "    def step(self):\n",
    "        if len(self.memory) < self.BATCH_SIZE:\n",
    "            return\n",
    "\n",
    "        states, actions, rewards, next_states, dones, old_log_probs, old_values = zip(*self.memory)\n",
    "        states = torch.tensor(np.array(states), dtype=torch.float32, device=self.device)\n",
    "        actions = torch.tensor(actions, dtype=torch.int64, device=self.device)\n",
    "        old_log_probs = torch.tensor(old_log_probs, dtype=torch.float32, device=self.device)\n",
    "        old_values = torch.tensor(old_values, dtype=torch.float32, device=self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            _, next_value = self.policy_net(states[-1:])\n",
    "        advantages, returns = self.compute_gae(rewards, old_values.tolist(), next_value.item(), dones)\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "        for _ in range(self.PPO_EPOCHS):\n",
    "            logits, values = self.policy_net(states)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            log_probs = F.log_softmax(logits, dim=-1)\n",
    "            selected_log_probs = log_probs[range(len(actions)), actions]\n",
    "\n",
    "            ratios = torch.exp(selected_log_probs - old_log_probs)\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1 - self.clip, 1 + self.clip) * advantages\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "            value_loss = F.mse_loss(values.squeeze(), returns)\n",
    "            entropy = -(probs * log_probs).sum(dim=-1).mean()\n",
    "\n",
    "            loss = policy_loss + self.VALUE_LOSS_COEF * value_loss - self.ENTROPY_COEF * entropy\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        self.memory.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DtwFDXwzd9hc"
   },
   "source": [
    "## Training Function Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "l4MUva-Md9hd"
   },
   "outputs": [],
   "source": [
    "def train_ppo(env, training_episodes=1000, agent_class=PPOAgent):\n",
    "    CONVERGENCE_REWARD_COUNT = 20\n",
    "    CONVERGENCE_MIN_REWARD = 270\n",
    "    \n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    \n",
    "    agent = agent_class(state_dim, action_dim)\n",
    "    rewards_per_episode = []\n",
    "    steps_per_episode = []\n",
    "    episode_times = []\n",
    "    start_episode = 0\n",
    "    early_stop_counter = 0\n",
    "\n",
    "    for episode in range(training_episodes):\n",
    "        start_time = time.time()\n",
    "        obs, _ = env.reset()\n",
    "        state = obs\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        steps_taken = 0\n",
    "        agent.memory = []\n",
    "\n",
    "        while not done:\n",
    "            action, log_prob, value = agent.select_action(state)\n",
    "            obs, reward, done, _, _ = env.step(action)\n",
    "            next_state = np.array(obs, dtype=np.float32)\n",
    "            total_reward += reward\n",
    "            steps_taken += 1\n",
    "\n",
    "            agent.store_transition(state, action, reward, next_state, done, log_prob, value)\n",
    "            state = next_state\n",
    "\n",
    "        average_rewards_over_last_episodes = np.mean(rewards_per_episode[-CONVERGENCE_REWARD_COUNT:])\n",
    "          \n",
    "        if average_rewards_over_last_episodes >= CONVERGENCE_MIN_REWARD and total_reward >= CONVERGENCE_MIN_REWARD:\n",
    "            print(\"Early stopping condition met! Stopping training.\")\n",
    "            print(f\"Episode {episode + 1:}: Total Reward: {total_reward:.3f}, Steps: {steps_taken}, Time: {episode_time:.2f}s\")\n",
    "            break\n",
    "        \n",
    "        agent.step()\n",
    "        steps_per_episode.append(steps_taken)\n",
    "        rewards_per_episode.append(total_reward)\n",
    "        episode_time = time.time() - start_time\n",
    "        episode_times.append(episode_time)\n",
    "\n",
    "        if episode % 10 == 0:    \n",
    "            print(f\"Episode {episode + 1}: Total Reward: {total_reward}, Steps: {steps_taken}, Time: {episode_time:.2f}s\")\n",
    "        if episode % 100 == 0:\n",
    "            checkpoint = {\n",
    "              'policy_net_state_dict': agent.policy_net.state_dict(),\n",
    "              'optimizer_state_dict': agent.optimizer.state_dict(),\n",
    "              'rewards_per_episode': rewards_per_episode,\n",
    "              'episode_times': episode_times,\n",
    "              'steps_per_episode': steps_per_episode\n",
    "            }\n",
    "\n",
    "            torch.save(checkpoint, \"ppo_lunar_lander_v3_checkpoint.pth\")\n",
    "            torch.save(agent.policy_net.state_dict(), \"ppo_lunar_lander_v3.pth\")\n",
    "            print(\"Checkpoint Reached!\")\n",
    "   \n",
    "    return agent, rewards_per_episode, episode_times, steps_per_episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "403yYjelcdkY"
   },
   "source": [
    "## LunarLander-v3 - Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "SRvx7n1tcUle",
    "outputId": "dc39b3d7-a6c9-4760-b5f8-8acab80da245"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\viswa\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\viswa\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward: 21.13438299682754, Steps: 68, Time: 0.05s\n",
      "Checkpoint Reached!\n",
      "Episode 11: Total Reward: -239.87420604537428, Steps: 111, Time: 0.05s\n",
      "Episode 21: Total Reward: -136.7281862467937, Steps: 79, Time: 0.04s\n",
      "Episode 31: Total Reward: -85.33406590701898, Steps: 75, Time: 0.05s\n",
      "Episode 41: Total Reward: -241.60280256366542, Steps: 77, Time: 0.05s\n",
      "Episode 51: Total Reward: -556.6270955969788, Steps: 103, Time: 0.08s\n",
      "Episode 61: Total Reward: -349.7701775752238, Steps: 87, Time: 0.08s\n",
      "Episode 71: Total Reward: -228.28127840755192, Steps: 71, Time: 0.05s\n",
      "Episode 81: Total Reward: -327.2710407449675, Steps: 98, Time: 0.06s\n",
      "Episode 91: Total Reward: -100.57678295932203, Steps: 144, Time: 0.07s\n",
      "Episode 101: Total Reward: -371.0338577545905, Steps: 96, Time: 0.06s\n",
      "Checkpoint Reached!\n",
      "Episode 111: Total Reward: -200.55970007982734, Steps: 83, Time: 0.06s\n",
      "Episode 121: Total Reward: -435.9761299478767, Steps: 256, Time: 0.13s\n",
      "Episode 131: Total Reward: -367.7972854758757, Steps: 122, Time: 0.07s\n",
      "Episode 141: Total Reward: -340.9232180539809, Steps: 113, Time: 0.07s\n",
      "Episode 151: Total Reward: -496.0448581467719, Steps: 128, Time: 0.08s\n",
      "Episode 161: Total Reward: -578.4730192829926, Steps: 73, Time: 0.05s\n",
      "Episode 171: Total Reward: -391.0561865283833, Steps: 60, Time: 0.05s\n",
      "Episode 181: Total Reward: -477.8644325573608, Steps: 58, Time: 0.06s\n",
      "Episode 191: Total Reward: -319.3646404088403, Steps: 71, Time: 0.05s\n",
      "Episode 201: Total Reward: -433.29824171319495, Steps: 103, Time: 0.07s\n",
      "Checkpoint Reached!\n",
      "Episode 211: Total Reward: -245.25758566068578, Steps: 83, Time: 0.07s\n",
      "Episode 221: Total Reward: -698.7186748585868, Steps: 94, Time: 0.07s\n",
      "Episode 231: Total Reward: -405.4500825069281, Steps: 81, Time: 0.07s\n",
      "Episode 241: Total Reward: -514.1358795034984, Steps: 77, Time: 0.05s\n",
      "Episode 251: Total Reward: -471.9335944897248, Steps: 71, Time: 0.07s\n",
      "Episode 261: Total Reward: -377.90298220309074, Steps: 49, Time: 0.04s\n",
      "Episode 271: Total Reward: -953.3375292439669, Steps: 94, Time: 0.07s\n",
      "Episode 281: Total Reward: -753.6759848879535, Steps: 84, Time: 0.07s\n",
      "Episode 291: Total Reward: -860.8842324134995, Steps: 116, Time: 0.09s\n",
      "Episode 301: Total Reward: -624.1183863463476, Steps: 77, Time: 0.06s\n",
      "Checkpoint Reached!\n",
      "Episode 311: Total Reward: -500.1260410600605, Steps: 72, Time: 0.07s\n",
      "Episode 321: Total Reward: -1029.1927231076934, Steps: 141, Time: 0.11s\n",
      "Episode 331: Total Reward: -1054.9027159334992, Steps: 149, Time: 0.08s\n",
      "Episode 341: Total Reward: -729.8803071747022, Steps: 105, Time: 0.08s\n",
      "Episode 351: Total Reward: -1225.6368905969819, Steps: 174, Time: 0.13s\n",
      "Episode 361: Total Reward: -834.5220011239782, Steps: 122, Time: 0.10s\n",
      "Episode 371: Total Reward: -486.3228481627395, Steps: 78, Time: 0.08s\n",
      "Episode 381: Total Reward: -536.2731394184027, Steps: 97, Time: 0.06s\n",
      "Episode 391: Total Reward: -587.6257009442788, Steps: 94, Time: 0.07s\n",
      "Episode 401: Total Reward: -834.1056281985268, Steps: 124, Time: 0.10s\n",
      "Checkpoint Reached!\n",
      "Episode 411: Total Reward: -379.57368823024797, Steps: 79, Time: 0.09s\n",
      "Episode 421: Total Reward: -581.7114156276206, Steps: 83, Time: 0.06s\n",
      "Episode 431: Total Reward: -531.6857783067676, Steps: 77, Time: 0.06s\n",
      "Episode 441: Total Reward: -828.4902903233415, Steps: 159, Time: 0.10s\n",
      "Episode 451: Total Reward: -1139.2900441434513, Steps: 149, Time: 0.12s\n",
      "Episode 461: Total Reward: -693.1125655357978, Steps: 87, Time: 0.07s\n",
      "Episode 471: Total Reward: -3886.031791331526, Steps: 397, Time: 0.27s\n",
      "Episode 481: Total Reward: -711.8497404149829, Steps: 87, Time: 0.07s\n",
      "Episode 491: Total Reward: -642.3153594431608, Steps: 88, Time: 0.08s\n",
      "Episode 501: Total Reward: -615.8990883274291, Steps: 73, Time: 0.08s\n",
      "Checkpoint Reached!\n",
      "Episode 511: Total Reward: -412.81449501733704, Steps: 92, Time: 0.10s\n",
      "Episode 521: Total Reward: -738.9714705034536, Steps: 110, Time: 0.09s\n",
      "Episode 531: Total Reward: -471.7307796582613, Steps: 84, Time: 0.05s\n",
      "Episode 541: Total Reward: -712.3985146501077, Steps: 92, Time: 0.10s\n",
      "Episode 551: Total Reward: -663.9374553588813, Steps: 111, Time: 0.08s\n",
      "Episode 561: Total Reward: -514.2070024607041, Steps: 97, Time: 0.07s\n",
      "Episode 571: Total Reward: -572.1960485173181, Steps: 78, Time: 0.08s\n",
      "Episode 581: Total Reward: -627.8897394904094, Steps: 106, Time: 0.10s\n",
      "Episode 591: Total Reward: -787.7570667219417, Steps: 116, Time: 0.11s\n",
      "Episode 601: Total Reward: -581.7380596147739, Steps: 77, Time: 0.06s\n",
      "Checkpoint Reached!\n",
      "Episode 611: Total Reward: -2122.5770172064094, Steps: 242, Time: 0.15s\n",
      "Episode 621: Total Reward: -1091.1250901829305, Steps: 147, Time: 0.11s\n",
      "Episode 631: Total Reward: -630.9548085680988, Steps: 137, Time: 0.08s\n",
      "Episode 641: Total Reward: -515.9849030056527, Steps: 83, Time: 0.08s\n",
      "Episode 651: Total Reward: -556.4044801301262, Steps: 120, Time: 0.08s\n",
      "Episode 661: Total Reward: -860.0386794141332, Steps: 119, Time: 0.07s\n",
      "Episode 671: Total Reward: -505.67279511576993, Steps: 103, Time: 0.09s\n",
      "Episode 681: Total Reward: -724.2618127595711, Steps: 143, Time: 0.10s\n",
      "Episode 691: Total Reward: -521.8444606473363, Steps: 85, Time: 0.08s\n",
      "Episode 701: Total Reward: -410.44149397115586, Steps: 79, Time: 0.08s\n",
      "Checkpoint Reached!\n",
      "Episode 711: Total Reward: -484.87281810124654, Steps: 81, Time: 0.05s\n",
      "Episode 721: Total Reward: -785.5600350983201, Steps: 113, Time: 0.08s\n",
      "Episode 731: Total Reward: -697.5573109801892, Steps: 97, Time: 0.09s\n",
      "Episode 741: Total Reward: -1127.3910896133996, Steps: 145, Time: 0.11s\n",
      "Episode 751: Total Reward: -1475.5194628563336, Steps: 212, Time: 0.14s\n",
      "Episode 761: Total Reward: -560.0825029974488, Steps: 84, Time: 0.05s\n",
      "Episode 771: Total Reward: -1493.9773868421219, Steps: 203, Time: 0.14s\n",
      "Episode 781: Total Reward: -449.4070484732855, Steps: 87, Time: 0.07s\n",
      "Episode 791: Total Reward: -1329.8198066520054, Steps: 175, Time: 0.11s\n",
      "Episode 801: Total Reward: -667.4568018786006, Steps: 102, Time: 0.09s\n",
      "Checkpoint Reached!\n",
      "Episode 811: Total Reward: -742.4262701639225, Steps: 137, Time: 0.09s\n",
      "Episode 821: Total Reward: -803.2819120810885, Steps: 122, Time: 0.11s\n",
      "Episode 831: Total Reward: -724.6832895781309, Steps: 95, Time: 0.08s\n",
      "Episode 841: Total Reward: -1414.2984122601376, Steps: 204, Time: 0.13s\n",
      "Episode 851: Total Reward: -706.5221468238036, Steps: 97, Time: 0.05s\n",
      "Episode 861: Total Reward: -1261.794001169383, Steps: 171, Time: 0.12s\n",
      "Episode 871: Total Reward: -552.145208144246, Steps: 83, Time: 0.07s\n",
      "Episode 881: Total Reward: -531.648118688677, Steps: 75, Time: 0.08s\n",
      "Episode 891: Total Reward: -593.2887306522803, Steps: 82, Time: 0.08s\n",
      "Episode 901: Total Reward: -482.31699302750354, Steps: 82, Time: 0.07s\n",
      "Checkpoint Reached!\n",
      "Episode 911: Total Reward: -527.0115759293083, Steps: 87, Time: 0.07s\n",
      "Episode 921: Total Reward: -1085.9926584734662, Steps: 169, Time: 0.11s\n",
      "Episode 931: Total Reward: -430.3460180965114, Steps: 80, Time: 0.08s\n",
      "Episode 941: Total Reward: -514.8699187602695, Steps: 76, Time: 0.05s\n",
      "Episode 951: Total Reward: -844.2122304162751, Steps: 109, Time: 0.09s\n",
      "Episode 961: Total Reward: -442.1974103380193, Steps: 84, Time: 0.08s\n",
      "Episode 971: Total Reward: -528.1506433282873, Steps: 92, Time: 0.07s\n",
      "Episode 981: Total Reward: -1137.185382560944, Steps: 187, Time: 0.10s\n",
      "Episode 991: Total Reward: -339.53325090213934, Steps: 76, Time: 0.08s\n",
      "Episode 1001: Total Reward: -1563.390888986682, Steps: 192, Time: 0.14s\n",
      "Checkpoint Reached!\n",
      "Episode 1011: Total Reward: -464.65345330230855, Steps: 72, Time: 0.08s\n",
      "Episode 1021: Total Reward: -919.7138201458268, Steps: 124, Time: 0.11s\n",
      "Episode 1031: Total Reward: -631.5167391499515, Steps: 98, Time: 0.08s\n",
      "Episode 1041: Total Reward: -2551.7139322430917, Steps: 273, Time: 0.19s\n",
      "Episode 1051: Total Reward: -812.9728808391744, Steps: 120, Time: 0.09s\n",
      "Episode 1061: Total Reward: -513.5865779657641, Steps: 94, Time: 0.07s\n",
      "Episode 1071: Total Reward: -1235.0725870894119, Steps: 167, Time: 0.13s\n",
      "Episode 1081: Total Reward: -805.4205496841728, Steps: 124, Time: 0.09s\n",
      "Episode 1091: Total Reward: -433.47394477546015, Steps: 83, Time: 0.06s\n",
      "Episode 1101: Total Reward: -490.20903086292526, Steps: 72, Time: 0.06s\n",
      "Checkpoint Reached!\n",
      "Episode 1111: Total Reward: -699.2667076850952, Steps: 93, Time: 0.07s\n",
      "Episode 1121: Total Reward: -423.85310072895453, Steps: 99, Time: 0.10s\n",
      "Episode 1131: Total Reward: -671.9746682442284, Steps: 90, Time: 0.08s\n",
      "Episode 1141: Total Reward: -1078.2867913209573, Steps: 192, Time: 0.14s\n",
      "Episode 1151: Total Reward: -618.8424582380891, Steps: 93, Time: 0.07s\n",
      "Episode 1161: Total Reward: -544.749063260919, Steps: 88, Time: 0.08s\n",
      "Episode 1171: Total Reward: -609.7906486958713, Steps: 88, Time: 0.09s\n",
      "Episode 1181: Total Reward: -607.0678468012255, Steps: 98, Time: 0.08s\n",
      "Episode 1191: Total Reward: -452.05206682844516, Steps: 85, Time: 0.06s\n",
      "Episode 1201: Total Reward: -512.3994929492263, Steps: 76, Time: 0.07s\n",
      "Checkpoint Reached!\n",
      "Episode 1211: Total Reward: -560.7601089416553, Steps: 77, Time: 0.05s\n",
      "Episode 1221: Total Reward: -2248.625033672434, Steps: 271, Time: 0.16s\n",
      "Episode 1231: Total Reward: -537.674596148069, Steps: 107, Time: 0.09s\n",
      "Episode 1241: Total Reward: -3170.1679716789713, Steps: 305, Time: 0.23s\n",
      "Episode 1251: Total Reward: -693.5185535999005, Steps: 87, Time: 0.09s\n",
      "Episode 1261: Total Reward: -777.407313696975, Steps: 144, Time: 0.12s\n",
      "Episode 1271: Total Reward: -645.767568589052, Steps: 102, Time: 0.09s\n",
      "Episode 1281: Total Reward: -1291.2720070143812, Steps: 186, Time: 0.16s\n",
      "Episode 1291: Total Reward: -522.2206216175978, Steps: 78, Time: 0.07s\n",
      "Episode 1301: Total Reward: -1532.1179272301458, Steps: 185, Time: 0.14s\n",
      "Checkpoint Reached!\n",
      "Episode 1311: Total Reward: -553.6987772205687, Steps: 76, Time: 0.07s\n",
      "Episode 1321: Total Reward: -911.2638854729331, Steps: 140, Time: 0.09s\n",
      "Episode 1331: Total Reward: -808.0505110683833, Steps: 127, Time: 0.08s\n",
      "Episode 1341: Total Reward: -650.3178240214236, Steps: 115, Time: 0.11s\n",
      "Episode 1351: Total Reward: -744.5656335503729, Steps: 123, Time: 0.10s\n",
      "Episode 1361: Total Reward: -1267.5036282837223, Steps: 155, Time: 0.13s\n",
      "Episode 1371: Total Reward: -504.73186635833565, Steps: 99, Time: 0.09s\n",
      "Episode 1381: Total Reward: -1574.4488570844062, Steps: 218, Time: 0.14s\n",
      "Episode 1391: Total Reward: -785.5928004827209, Steps: 110, Time: 0.11s\n",
      "Episode 1401: Total Reward: -971.9513967432935, Steps: 125, Time: 0.11s\n",
      "Checkpoint Reached!\n",
      "Episode 1411: Total Reward: -2009.0118232253387, Steps: 242, Time: 0.16s\n",
      "Episode 1421: Total Reward: -1480.982852824737, Steps: 200, Time: 0.14s\n",
      "Episode 1431: Total Reward: -1177.4360585962895, Steps: 159, Time: 0.14s\n",
      "Episode 1441: Total Reward: -847.3904273444161, Steps: 115, Time: 0.09s\n",
      "Episode 1451: Total Reward: -795.0094906145868, Steps: 137, Time: 0.10s\n",
      "Episode 1461: Total Reward: -817.4803471058892, Steps: 112, Time: 0.10s\n",
      "Episode 1471: Total Reward: -2258.693563223969, Steps: 256, Time: 0.20s\n",
      "Episode 1481: Total Reward: -693.8582066576643, Steps: 103, Time: 0.09s\n",
      "Episode 1491: Total Reward: -634.9779508306079, Steps: 77, Time: 0.07s\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 7\u001b[0m\n\u001b[0;32m      2\u001b[0m lunar_env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(env_name)\n\u001b[0;32m      4\u001b[0m trained_agent, rewards_per_episode, episode_times, steps_per_episode \u001b[38;5;241m=\u001b[39m train_ppo(lunar_env, training_episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1500\u001b[39m)\n\u001b[0;32m      6\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpolicy_net_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43magent\u001b[49m\u001b[38;5;241m.\u001b[39mpolicy_net\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: trained_agent\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrewards_per_episode\u001b[39m\u001b[38;5;124m'\u001b[39m: rewards_per_episode,\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepisode_times\u001b[39m\u001b[38;5;124m'\u001b[39m: episode_times,\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msteps_per_episode\u001b[39m\u001b[38;5;124m'\u001b[39m: steps_per_episode\n\u001b[0;32m     12\u001b[0m }\n\u001b[0;32m     14\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(checkpoint, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mppo_lunar_lander_v3_checkpoint.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(trained_agent\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mppo_lunar_lander_v3.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'agent' is not defined"
     ]
    }
   ],
   "source": [
    "env_name = \"LunarLander-v3\"\n",
    "lunar_env = gym.make(env_name)\n",
    "\n",
    "trained_agent, rewards_per_episode, episode_times, steps_per_episode = train_ppo(lunar_env, training_episodes = 1500)\n",
    "\n",
    "checkpoint = {\n",
    "    'policy_net_state_dict': trained_agent.policy_net.state_dict(),\n",
    "    'optimizer_state_dict': trained_agent.optimizer.state_dict(),\n",
    "    'rewards_per_episode': rewards_per_episode,\n",
    "    'episode_times': episode_times,\n",
    "    'steps_per_episode': steps_per_episode\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, \"ppo_lunar_lander_v3_checkpoint.pth\")\n",
    "torch.save(trained_agent.model.state_dict(), \"ppo_lunar_lander_v3.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = torch.load(\"ppo_lunar_lander_v3_checkpoint.pth\", weights_only=False)\n",
    "\n",
    "rewards_per_episode = results[\"rewards_per_episode\"]\n",
    "steps_per_episode = results[\"steps_per_episode\"]\n",
    "\n",
    "plt.plot(rewards_per_episode)\n",
    "plt.title(\"Rewards Per Episode - Training\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Rewards\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.plot(steps_per_episode)\n",
    "plt.title(\"Steps Per Episode - Training\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Steps Taken\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LunarLander-v3 - Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_arr = []\n",
    "rewards_arr = []\n",
    "\n",
    "env_name = \"LunarLander-v3\"\n",
    "lunar_env = gym.make(env_name)\n",
    "\n",
    "state_dim = lunar_env.observation_space.shape[0]\n",
    "action_dim = lunar_env.action_space.n\n",
    "\n",
    "trained_agent = PPOAgent(state_dim, action_dim)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "trained_agent.model.load_state_dict(torch.load(\"ppo_lunar_lander_v3.pth\", map_location=device, weights_only=True))\n",
    "\n",
    "for episode in range(10):\n",
    "    obs, _ = lunar_env.reset()\n",
    "    state = obs\n",
    "    tot_reward = 0\n",
    "    episode_over = False\n",
    "    steps_taken = 0\n",
    "    \n",
    "    while not episode_over:\n",
    "        with torch.no_grad():\n",
    "            action, _, _ = trained_agent.select_action(state)\n",
    "        obs, reward, done, truncated, _ = lunar_env.step(action)\n",
    "        episode_over = done or truncated\n",
    "        state = np.array(obs, dtype=np.float32)\n",
    "        steps_taken +=1\n",
    "        tot_reward += reward\n",
    "    rewards_arr.append(tot_reward)\n",
    "    steps_arr.append(steps_taken)\n",
    "    print(f\"Episode {episode + 1}: Total Reward = {tot_reward}, Steps taken: {steps_taken}\")\n",
    "\n",
    "print(f\"\\nAverage reward over 10 episodes: {np.mean(rewards_arr)}\")\n",
    "print(f\"Average steps taken over 10 episodes: {np.mean(steps_arr)}\\n\")\n",
    "\n",
    "plt.plot(rewards_arr, marker=\"o\")\n",
    "plt.title(\"Rewards Per Episode - Evaluation\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Rewards\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.plot(steps_arr, marker=\"o\")\n",
    "plt.title(\"Steps Per Episode - Evaluation\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Steps Taken\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
