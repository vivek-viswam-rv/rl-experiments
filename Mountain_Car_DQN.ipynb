{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Mountain Car DQN"
      ],
      "metadata": {
        "id": "SFM11_y6mH_u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "n_e4ChUFmPY6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33NAxeazlaAS",
        "outputId": "82e81b47-455b-426b-9eb8-6a146135039a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: swig in /usr/local/lib/python3.11/dist-packages (4.3.1)\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.13.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Requirement already satisfied: box2d-py==2.3.5 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.3.5)\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.6.1)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.3.1)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "from abc import ABC, abstractmethod\n",
        "from enum import Enum\n",
        "from gymnasium import spaces\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from collections import deque\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "%pip install swig\n",
        "%pip install \"gymnasium[box2d]\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mounting Google Drive"
      ],
      "metadata": {
        "id": "ZvxoJCPtnuwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "DRIVE_PATH = '/content/drive/MyDrive/dqn_mountain_car_checkpoints/'\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "os.makedirs(DRIVE_PATH, exist_ok=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nr4oaoyJntcf",
        "outputId": "fa06b764-87d2-4c38-ecf7-947ce0e7df36"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Class definition"
      ],
      "metadata": {
        "id": "cPZkGCWNmbmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, action_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "\n",
        "        return self.fc3(x)\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.buffer, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "class DQNAgent:\n",
        "    BATCH_SIZE = 32\n",
        "    EPSILON_START = 1\n",
        "    EPSILON_CUTOFF = 0.05\n",
        "    EPSILON_DECAY = 0.995 # Will take about 1000 episodes to reach 0.1 and 1300 to reach 0.05\n",
        "    GAMMA = 0.99\n",
        "    LR = 5e-4\n",
        "    MEMORY_SIZE = 30000\n",
        "    total_steps = 0\n",
        "\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.policy_net = DQN(state_dim, action_dim).to(self.device)\n",
        "        self.target_net = DQN(state_dim, action_dim).to(self.device)\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "        self.target_net.eval()\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.LR)\n",
        "        self.memory = ReplayBuffer(self.MEMORY_SIZE)\n",
        "        self.steps_done = 0\n",
        "        self.epsilon = self.EPSILON_START\n",
        "\n",
        "    def select_e_greedy_action(self, env, state):\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randint(0, env.action_space.n - 1)\n",
        "        else:\n",
        "            state = torch.tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
        "            with torch.no_grad():\n",
        "                return self.policy_net(state).argmax(dim=1).item()\n",
        "\n",
        "    def select_greedy_action(self, state):\n",
        "      state = torch.tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
        "      with torch.no_grad():\n",
        "          return self.policy_net(state).argmax(dim=1).item()\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        self.total_steps += 1\n",
        "        self.epsilon = max(self.EPSILON_CUTOFF, self.EPSILON_START * (self.EPSILON_DECAY ** self.current_episode))\n",
        "\n",
        "    def step(self):\n",
        "        if len(self.memory) < self.BATCH_SIZE:\n",
        "            return\n",
        "        batch = self.memory.sample(self.BATCH_SIZE)\n",
        "\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "        states = torch.tensor(np.array(states), dtype=torch.float32, device=self.device)\n",
        "        actions = torch.tensor(np.array(actions), dtype=torch.int64, device=self.device).unsqueeze(1)\n",
        "        rewards = torch.tensor(np.array(rewards), dtype=torch.float32, device=self.device).unsqueeze(1)\n",
        "        next_states = torch.tensor(np.array(next_states), dtype=torch.float32, device=self.device)\n",
        "        dones = torch.tensor(np.array(dones), dtype=torch.float32, device=self.device).unsqueeze(1)\n",
        "        current_q = self.policy_net(states).gather(1, actions)\n",
        "        next_q = self.target_net(next_states).max(1, keepdim=True)[0].detach()\n",
        "        target_q = rewards + (self.GAMMA * next_q * (1 - dones))\n",
        "\n",
        "        loss = F.mse_loss(target_q, current_q)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def update_target_network(self):\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())"
      ],
      "metadata": {
        "id": "jjbfpeeImeIr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training function"
      ],
      "metadata": {
        "id": "Tk5hDKlcm0FW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_q_values(env, env_name, target_update_interval=10, training_episodes=1000, agent_class=DQNAgent, checkpoint_path=None):\n",
        "    MAX_STEPS = 200\n",
        "\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.n\n",
        "    agent = agent_class(state_dim, action_dim)\n",
        "    rewards_per_episode = []\n",
        "    epsilon_values = []\n",
        "    episode_times = []\n",
        "    start_episode = 0\n",
        "    early_stop_counter = 0\n",
        "\n",
        "    # Loading from last checkpoint in case training fails\n",
        "    if checkpoint_path is not None:\n",
        "        checkpoint = torch.load(os.path.join(DRIVE_PATH, checkpoint_path))\n",
        "        agent.policy_net.load_state_dict(checkpoint['policy_net_state_dict'])\n",
        "        agent.target_net.load_state_dict(checkpoint['policy_net_state_dict'])\n",
        "        agent.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        agent.total_steps = checkpoint['total_steps']\n",
        "        agent.current_episode = checkpoint['current_episode']\n",
        "        rewards_per_episode = checkpoint['rewards_per_episode']\n",
        "        epsilon_values = checkpoint['epsilon_values']\n",
        "        episode_times = checkpoint['episode_times']\n",
        "        start_episode = checkpoint['current_episode'] + 1\n",
        "        agent.epsilon = max(agent.EPSILON_CUTOFF, agent.EPSILON_START * (agent.EPSILON_DECAY ** agent.current_episode))\n",
        "        print(f\"Resumed training from episode {start_episode}\")\n",
        "\n",
        "    for episode in range(start_episode, training_episodes):\n",
        "        agent.current_episode = episode\n",
        "        start_time = time.time()\n",
        "        obs, _ = env.reset()\n",
        "        state = obs\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "        steps_taken = 0\n",
        "        episode_transitions = []\n",
        "\n",
        "        epsilon_values.append(agent.epsilon)\n",
        "\n",
        "        while not done and steps_taken < MAX_STEPS:\n",
        "            action = agent.select_e_greedy_action(env, state)\n",
        "            obs, reward, done, _, _ = env.step(action)\n",
        "            next_state = np.array(obs, dtype=np.float32)\n",
        "            episode_transitions.append((state, action, reward, next_state, done))\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "            steps_taken += 1\n",
        "\n",
        "            agent.decay_epsilon()\n",
        "\n",
        "        for transition in episode_transitions:\n",
        "            agent.memory.push(*transition)\n",
        "\n",
        "        for _ in range(len(episode_transitions)):\n",
        "            agent.step()\n",
        "\n",
        "        rewards_per_episode.append(total_reward)\n",
        "        end_time = time.time()\n",
        "        episode_time = end_time - start_time\n",
        "        episode_times.append(episode_time)\n",
        "\n",
        "\n",
        "        if episode % 10 == 0:\n",
        "            print(f\"Episode {episode + 1}: Total Reward: {total_reward}, Epsilon: {agent.epsilon:.3f}, Steps: {steps_taken}, Time: {episode_time:.2f}s\")\n",
        "            # Saving model\n",
        "            torch.save(agent.policy_net.state_dict(), os.path.join(DRIVE_PATH, \"dqn_mountain_car_env_v0.pth\"))\n",
        "            # Creating checkpoint dictionary and saving\n",
        "            checkpoint = {\n",
        "                'policy_net_state_dict': agent.policy_net.state_dict(),\n",
        "                'optimizer_state_dict': agent.optimizer.state_dict(),\n",
        "                'total_steps': agent.total_steps,\n",
        "                'current_episode': agent.current_episode,\n",
        "                'rewards_per_episode': rewards_per_episode,\n",
        "                'epsilon_values': epsilon_values,\n",
        "                'episode_times': episode_times\n",
        "            }\n",
        "            torch.save(checkpoint, os.path.join(DRIVE_PATH, \"dqn_mountain_car_checkpoints.pth\"))\n",
        "\n",
        "        if episode % target_update_interval == 0:\n",
        "            agent.update_target_network()\n",
        "\n",
        "    return agent, rewards_per_episode, epsilon_values, episode_times"
      ],
      "metadata": {
        "id": "M7xxQtzQm2AN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mountain Car"
      ],
      "metadata": {
        "id": "uv3APuaVmTuD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env_name = \"MountainCar-v0\"\n",
        "mountain_env = gym.make(env_name, render_mode=\"rgb_array\", goal_velocity=0.1)\n",
        "\n",
        "trained_agent, rewards_per_episode, epsilon_values, episode_times = train_q_values(mountain_env, env_name, training_episodes=1500)\n",
        "torch.save(trained_agent.policy_net.state_dict(), os.path.join(DRIVE_PATH, \"dqn_mountain_car_env_v0.pth\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXrtdzxtmFRc",
        "outputId": "b57e2d07-44e9-4dd5-87e3-99f9577d1227"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1: Total Reward: -200.0, Epsilon: 1.000, Steps: 200, Time: 1.91s\n",
            "Episode 11: Total Reward: -200.0, Epsilon: 0.951, Steps: 200, Time: 0.38s\n",
            "Episode 21: Total Reward: -200.0, Epsilon: 0.905, Steps: 200, Time: 0.55s\n",
            "Episode 31: Total Reward: -200.0, Epsilon: 0.860, Steps: 200, Time: 0.47s\n",
            "Episode 41: Total Reward: -200.0, Epsilon: 0.818, Steps: 200, Time: 0.40s\n",
            "Episode 51: Total Reward: -200.0, Epsilon: 0.778, Steps: 200, Time: 0.58s\n",
            "Episode 61: Total Reward: -200.0, Epsilon: 0.740, Steps: 200, Time: 0.40s\n",
            "Episode 71: Total Reward: -200.0, Epsilon: 0.704, Steps: 200, Time: 0.47s\n",
            "Episode 81: Total Reward: -200.0, Epsilon: 0.670, Steps: 200, Time: 0.55s\n",
            "Episode 91: Total Reward: -200.0, Epsilon: 0.637, Steps: 200, Time: 0.40s\n",
            "Episode 101: Total Reward: -200.0, Epsilon: 0.606, Steps: 200, Time: 0.40s\n",
            "Episode 111: Total Reward: -200.0, Epsilon: 0.576, Steps: 200, Time: 0.59s\n",
            "Episode 121: Total Reward: -200.0, Epsilon: 0.548, Steps: 200, Time: 0.41s\n",
            "Episode 131: Total Reward: -200.0, Epsilon: 0.521, Steps: 200, Time: 0.42s\n",
            "Episode 141: Total Reward: -200.0, Epsilon: 0.496, Steps: 200, Time: 0.59s\n",
            "Episode 151: Total Reward: -200.0, Epsilon: 0.471, Steps: 200, Time: 0.42s\n",
            "Episode 161: Total Reward: -200.0, Epsilon: 0.448, Steps: 200, Time: 0.43s\n",
            "Episode 171: Total Reward: -200.0, Epsilon: 0.427, Steps: 200, Time: 0.61s\n",
            "Episode 181: Total Reward: -200.0, Epsilon: 0.406, Steps: 200, Time: 0.42s\n",
            "Episode 191: Total Reward: -200.0, Epsilon: 0.386, Steps: 200, Time: 0.42s\n",
            "Episode 201: Total Reward: -200.0, Epsilon: 0.367, Steps: 200, Time: 0.55s\n",
            "Episode 211: Total Reward: -200.0, Epsilon: 0.349, Steps: 200, Time: 0.44s\n",
            "Episode 221: Total Reward: -200.0, Epsilon: 0.332, Steps: 200, Time: 0.44s\n",
            "Episode 231: Total Reward: -200.0, Epsilon: 0.316, Steps: 200, Time: 0.60s\n",
            "Episode 241: Total Reward: -200.0, Epsilon: 0.300, Steps: 200, Time: 0.47s\n",
            "Episode 251: Total Reward: -200.0, Epsilon: 0.286, Steps: 200, Time: 0.43s\n",
            "Episode 261: Total Reward: -200.0, Epsilon: 0.272, Steps: 200, Time: 0.67s\n",
            "Episode 271: Total Reward: -200.0, Epsilon: 0.258, Steps: 200, Time: 0.43s\n",
            "Episode 281: Total Reward: -200.0, Epsilon: 0.246, Steps: 200, Time: 0.45s\n",
            "Episode 291: Total Reward: -200.0, Epsilon: 0.234, Steps: 200, Time: 0.44s\n",
            "Episode 301: Total Reward: -200.0, Epsilon: 0.222, Steps: 200, Time: 0.44s\n",
            "Episode 311: Total Reward: -200.0, Epsilon: 0.211, Steps: 200, Time: 0.56s\n",
            "Episode 321: Total Reward: -200.0, Epsilon: 0.201, Steps: 200, Time: 0.45s\n",
            "Episode 331: Total Reward: -200.0, Epsilon: 0.191, Steps: 200, Time: 0.45s\n",
            "Episode 341: Total Reward: -200.0, Epsilon: 0.182, Steps: 200, Time: 0.62s\n",
            "Episode 351: Total Reward: -200.0, Epsilon: 0.173, Steps: 200, Time: 0.46s\n",
            "Episode 361: Total Reward: -200.0, Epsilon: 0.165, Steps: 200, Time: 0.46s\n",
            "Episode 371: Total Reward: -200.0, Epsilon: 0.157, Steps: 200, Time: 0.51s\n",
            "Episode 381: Total Reward: -200.0, Epsilon: 0.149, Steps: 200, Time: 0.45s\n",
            "Episode 391: Total Reward: -200.0, Epsilon: 0.142, Steps: 200, Time: 0.48s\n",
            "Episode 401: Total Reward: -200.0, Epsilon: 0.135, Steps: 200, Time: 0.46s\n",
            "Episode 411: Total Reward: -200.0, Epsilon: 0.128, Steps: 200, Time: 0.45s\n",
            "Episode 421: Total Reward: -200.0, Epsilon: 0.122, Steps: 200, Time: 0.63s\n",
            "Episode 431: Total Reward: -200.0, Epsilon: 0.116, Steps: 200, Time: 0.47s\n",
            "Episode 441: Total Reward: -200.0, Epsilon: 0.110, Steps: 200, Time: 0.46s\n",
            "Episode 451: Total Reward: -200.0, Epsilon: 0.105, Steps: 200, Time: 0.48s\n",
            "Episode 461: Total Reward: -200.0, Epsilon: 0.100, Steps: 200, Time: 0.54s\n",
            "Episode 471: Total Reward: -200.0, Epsilon: 0.095, Steps: 200, Time: 0.57s\n",
            "Episode 481: Total Reward: -200.0, Epsilon: 0.090, Steps: 200, Time: 0.46s\n",
            "Episode 491: Total Reward: -200.0, Epsilon: 0.086, Steps: 200, Time: 0.47s\n",
            "Episode 501: Total Reward: -200.0, Epsilon: 0.082, Steps: 200, Time: 0.80s\n",
            "Episode 511: Total Reward: -200.0, Epsilon: 0.078, Steps: 200, Time: 0.48s\n",
            "Episode 521: Total Reward: -200.0, Epsilon: 0.074, Steps: 200, Time: 0.55s\n",
            "Episode 531: Total Reward: -200.0, Epsilon: 0.070, Steps: 200, Time: 0.47s\n",
            "Episode 541: Total Reward: -200.0, Epsilon: 0.067, Steps: 200, Time: 0.51s\n",
            "Episode 551: Total Reward: -200.0, Epsilon: 0.063, Steps: 200, Time: 0.65s\n",
            "Episode 561: Total Reward: -200.0, Epsilon: 0.060, Steps: 200, Time: 0.46s\n",
            "Episode 571: Total Reward: -200.0, Epsilon: 0.057, Steps: 200, Time: 0.50s\n",
            "Episode 581: Total Reward: -200.0, Epsilon: 0.055, Steps: 200, Time: 0.50s\n",
            "Episode 591: Total Reward: -200.0, Epsilon: 0.052, Steps: 200, Time: 0.50s\n",
            "Episode 601: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.65s\n",
            "Episode 611: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.49s\n",
            "Episode 621: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.50s\n",
            "Episode 631: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.69s\n",
            "Episode 641: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.48s\n",
            "Episode 651: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.51s\n",
            "Episode 661: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.54s\n",
            "Episode 671: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.49s\n",
            "Episode 681: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.72s\n",
            "Episode 691: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.49s\n",
            "Episode 701: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.49s\n",
            "Episode 711: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.49s\n",
            "Episode 721: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.49s\n",
            "Episode 731: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.70s\n",
            "Episode 741: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.47s\n",
            "Episode 751: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.52s\n",
            "Episode 761: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.49s\n",
            "Episode 771: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.48s\n",
            "Episode 781: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.73s\n",
            "Episode 791: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.50s\n",
            "Episode 801: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.49s\n",
            "Episode 811: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.52s\n",
            "Episode 821: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.52s\n",
            "Episode 831: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.70s\n",
            "Episode 841: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.48s\n",
            "Episode 851: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.48s\n",
            "Episode 861: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.49s\n",
            "Episode 871: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.49s\n",
            "Episode 881: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.69s\n",
            "Episode 891: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.48s\n",
            "Episode 901: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.49s\n",
            "Episode 911: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.71s\n",
            "Episode 921: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.58s\n",
            "Episode 931: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.50s\n",
            "Episode 941: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.48s\n",
            "Episode 951: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.49s\n",
            "Episode 961: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.64s\n",
            "Episode 971: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.48s\n",
            "Episode 981: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.50s\n",
            "Episode 991: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.49s\n",
            "Episode 1001: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.48s\n",
            "Episode 1011: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.72s\n",
            "Episode 1021: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.49s\n",
            "Episode 1031: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.48s\n",
            "Episode 1041: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.48s\n",
            "Episode 1051: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.50s\n",
            "Episode 1061: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.73s\n",
            "Episode 1071: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.48s\n",
            "Episode 1081: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 1.07s\n",
            "Episode 1091: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.49s\n",
            "Episode 1101: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.53s\n",
            "Episode 1111: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.72s\n",
            "Episode 1121: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.61s\n",
            "Episode 1131: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.52s\n",
            "Episode 1141: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.50s\n",
            "Episode 1151: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.50s\n",
            "Episode 1161: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.67s\n",
            "Episode 1171: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.49s\n",
            "Episode 1181: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.49s\n",
            "Episode 1191: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.50s\n",
            "Episode 1201: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.51s\n",
            "Episode 1211: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.68s\n",
            "Episode 1221: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.48s\n",
            "Episode 1231: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.50s\n",
            "Episode 1241: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.49s\n",
            "Episode 1251: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.58s\n",
            "Episode 1261: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.70s\n",
            "Episode 1271: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.49s\n",
            "Episode 1281: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.48s\n",
            "Episode 1291: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.50s\n",
            "Episode 1301: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.53s\n",
            "Episode 1311: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.73s\n",
            "Episode 1321: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.49s\n",
            "Episode 1331: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.49s\n",
            "Episode 1341: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.49s\n",
            "Episode 1351: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.50s\n",
            "Episode 1361: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.70s\n",
            "Episode 1371: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.48s\n",
            "Episode 1381: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.47s\n",
            "Episode 1391: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.63s\n",
            "Episode 1401: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.50s\n",
            "Episode 1411: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.62s\n",
            "Episode 1421: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.48s\n",
            "Episode 1431: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.50s\n",
            "Episode 1441: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.68s\n",
            "Episode 1451: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.50s\n",
            "Episode 1461: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.50s\n",
            "Episode 1471: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.52s\n",
            "Episode 1481: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.55s\n",
            "Episode 1491: Total Reward: -200.0, Epsilon: 0.050, Steps: 200, Time: 0.65s\n"
          ]
        }
      ]
    }
  ]
}