{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFM11_y6mH_u"
   },
   "source": [
    "# Mountain Car DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n_e4ChUFmPY6"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "33NAxeazlaAS",
    "outputId": "82e81b47-455b-426b-9eb8-6a146135039a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: swig in c:\\users\\viswa\\anaconda3\\lib\\site-packages (4.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium[box2d] in c:\\users\\viswa\\anaconda3\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\viswa\\anaconda3\\lib\\site-packages (from gymnasium[box2d]) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\viswa\\anaconda3\\lib\\site-packages (from gymnasium[box2d]) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\viswa\\anaconda3\\lib\\site-packages (from gymnasium[box2d]) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\viswa\\anaconda3\\lib\\site-packages (from gymnasium[box2d]) (0.0.4)\n",
      "Requirement already satisfied: box2d-py==2.3.5 in c:\\users\\viswa\\anaconda3\\lib\\site-packages (from gymnasium[box2d]) (2.3.5)\n",
      "Requirement already satisfied: pygame>=2.1.3 in c:\\users\\viswa\\anaconda3\\lib\\site-packages (from gymnasium[box2d]) (2.6.1)\n",
      "Requirement already satisfied: swig==4.* in c:\\users\\viswa\\anaconda3\\lib\\site-packages (from gymnasium[box2d]) (4.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from abc import ABC, abstractmethod\n",
    "from enum import Enum\n",
    "from gymnasium import spaces\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "%pip install swig\n",
    "%pip install \"gymnasium[box2d]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cPZkGCWNmbmf"
   },
   "source": [
    "## Class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "jjbfpeeImeIr"
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 12)\n",
    "        self.fc2 = nn.Linear(12, 8)\n",
    "        self.fc3 = nn.Linear(8, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        return self.fc3(x)\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# DQN Agent\n",
    "class DQNAgent:\n",
    "    BATCH_SIZE = 32\n",
    "    EPSILON_START = 1\n",
    "    EPSILON_CUTOFF = 0.0\n",
    "    EPSILON_DECAY = 0.987\n",
    "    GAMMA = 0.9\n",
    "    LR = 75e-5\n",
    "    MEMORY_SIZE = 50000\n",
    "\n",
    "    def __init__(self, env):\n",
    "        state_dim = env.observation_space.shape[0]\n",
    "        action_dim = env.action_space.n\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.policy_net = DQN(state_dim, action_dim).to(self.device)\n",
    "        self.target_net = DQN(state_dim, action_dim).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.LR)\n",
    "        self.memory = ReplayBuffer(self.MEMORY_SIZE)\n",
    "        self.epsilon = self.EPSILON_START\n",
    "        self.total_steps = 0\n",
    "\n",
    "    def select_e_greedy_action(self, env, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, env.action_space.n - 1)\n",
    "        else:\n",
    "            state = torch.tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                return self.policy_net(state).argmax(dim=1).item()\n",
    "\n",
    "    def select_greedy_action(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            return self.policy_net(state).argmax(dim=1).item()\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.total_steps += 1\n",
    "        self.epsilon = max(self.EPSILON_CUTOFF, self.epsilon * self.EPSILON_DECAY)\n",
    "\n",
    "    def step(self):\n",
    "        if len(self.memory) < self.BATCH_SIZE:\n",
    "            return\n",
    "        batch = self.memory.sample(self.BATCH_SIZE)\n",
    "\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        states = torch.tensor(np.array(states), dtype=torch.float32, device=self.device)\n",
    "        actions = torch.tensor(np.array(actions), dtype=torch.int64, device=self.device).unsqueeze(1)\n",
    "        rewards = torch.tensor(np.array(rewards), dtype=torch.float32, device=self.device).unsqueeze(1)\n",
    "        next_states = torch.tensor(np.array(next_states), dtype=torch.float32, device=self.device)\n",
    "        dones = torch.tensor(np.array(dones), dtype=torch.float32, device=self.device).unsqueeze(1)\n",
    "        current_q = self.policy_net(states).gather(1, actions)\n",
    "        next_q = self.target_net(next_states).max(1, keepdim=True)[0].detach()\n",
    "        target_q = rewards + (self.GAMMA * next_q * (1 - dones))\n",
    "\n",
    "        loss = F.mse_loss(target_q, current_q)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tk5hDKlcm0FW"
   },
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "M7xxQtzQm2AN"
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_q_values(env, env_name, target_update_interval=100, training_episodes=1000, agent_class=DQNAgent):\n",
    "    OBS_HIGH_VALUES = env.observation_space.high\n",
    "    CONVERGENCE_TOLERANCE = 0.01\n",
    "    \n",
    "    agent = agent_class(env)\n",
    "    rewards_per_episode = []\n",
    "    steps_per_episode = []\n",
    "    epsilon_values = []\n",
    "    episode_times = []\n",
    "    start_episode = 0\n",
    "    early_stop_counter = 0\n",
    "\n",
    "    for episode in range(start_episode, training_episodes):\n",
    "        start_time = time.time()\n",
    "        obs, _ = env.reset()\n",
    "        state = obs\n",
    "        total_reward = 0\n",
    "        episode_over = False\n",
    "        steps_taken = 0\n",
    "        episode_transitions = []\n",
    "\n",
    "        epsilon_values.append(agent.epsilon)\n",
    "\n",
    "        while not episode_over:\n",
    "            action = agent.select_e_greedy_action(env, state)\n",
    "            obs, reward, done, truncated, _ = env.step(action)\n",
    "            reward += 2 ** (abs(state[1]) * 100)\n",
    "            reward += 10000000 if state[0] >= np.max(OBS_HIGH_VALUES[0]) else 0\n",
    "            next_state = np.array(obs, dtype=np.float32)\n",
    "            agent.memory.push(state, action, reward, next_state, done)\n",
    "            episode_over = done or truncated\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            steps_taken += 1\n",
    "            \n",
    "            agent.step()\n",
    "            \n",
    "            if agent.total_steps % target_update_interval == 0:\n",
    "                agent.update_target_network()\n",
    "\n",
    "        if steps_taken <= 110 and abs(agent.epsilon - agent.EPSILON_CUTOFF) <= CONVERGENCE_TOLERANCE:\n",
    "            print(\"Early stopping condition met! Stopping training.\")\n",
    "            print(f\"Episode {episode + 1:}: Total Reward: {total_reward:.3f}, Epsilon: {agent.epsilon:.3f}, Steps: {steps_taken}, Time: {episode_time:.2f}s\")\n",
    "            break\n",
    "        \n",
    "        agent.decay_epsilon()\n",
    "        rewards_per_episode.append(total_reward)\n",
    "        steps_per_episode.append(steps_taken)\n",
    "        end_time = time.time()\n",
    "        episode_time = end_time - start_time\n",
    "        episode_times.append(episode_time)\n",
    "\n",
    "        if episode % 10 == 0:\n",
    "            print(f\"Episode {episode + 1:}: Total Reward: {total_reward:.3f}, Epsilon: {agent.epsilon:.3f}, Steps: {steps_taken}, Time: {episode_time:.2f}s\")\n",
    "        if episode % 100 == 0:\n",
    "            # Saving model\n",
    "            checkpoint = {\n",
    "                'policy_net_state_dict': agent.policy_net.state_dict(),\n",
    "                'optimizer_state_dict': agent.optimizer.state_dict(),\n",
    "                'total_steps': agent.total_steps,\n",
    "                'current_episode': episode,\n",
    "                'rewards_per_episode': rewards_per_episode,\n",
    "                'epsilon_values': epsilon_values,\n",
    "                'episode_times': episode_times,\n",
    "                'steps_per_episode': steps_per_episode\n",
    "            }\n",
    "            torch.save(checkpoint, \"dqn_mountain_car_env_v0_checkpoint.pth\")\n",
    "            torch.save(agent.policy_net.state_dict(), \"dqn_mountain_car_env_v0.pth\")\n",
    "            print(\"Checkpoint Reached!\")\n",
    "    \n",
    "    return agent, rewards_per_episode, epsilon_values, episode_times, steps_per_episode\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uv3APuaVmTuD"
   },
   "source": [
    "## Mountain Car Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sXrtdzxtmFRc",
    "outputId": "b57e2d07-44e9-4dd5-87e3-99f9577d1227",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward: 638.336, Epsilon: 0.987, Steps: 1000, Time: 1.54s\n",
      "Checkpoint Reached!\n",
      "Episode 11: Total Reward: 3664.634, Epsilon: 0.866, Steps: 1000, Time: 1.51s\n",
      "Episode 21: Total Reward: 616.177, Epsilon: 0.760, Steps: 1000, Time: 1.54s\n",
      "Episode 31: Total Reward: 1997.453, Epsilon: 0.667, Steps: 1000, Time: 1.62s\n",
      "Episode 41: Total Reward: 5179.178, Epsilon: 0.585, Steps: 1000, Time: 1.59s\n",
      "Episode 51: Total Reward: 3577.151, Epsilon: 0.513, Steps: 866, Time: 1.61s\n",
      "Episode 61: Total Reward: 4241.324, Epsilon: 0.450, Steps: 1000, Time: 1.63s\n",
      "Episode 71: Total Reward: 2731.526, Epsilon: 0.395, Steps: 775, Time: 1.34s\n",
      "Episode 81: Total Reward: 8394.433, Epsilon: 0.346, Steps: 1000, Time: 1.59s\n",
      "Episode 91: Total Reward: 3864.778, Epsilon: 0.304, Steps: 1000, Time: 1.63s\n",
      "Episode 101: Total Reward: 3036.201, Epsilon: 0.267, Steps: 1000, Time: 1.80s\n",
      "Checkpoint Reached!\n",
      "Episode 111: Total Reward: 1884.660, Epsilon: 0.234, Steps: 168, Time: 0.27s\n",
      "Episode 121: Total Reward: 2702.131, Epsilon: 0.205, Steps: 253, Time: 0.57s\n",
      "Episode 131: Total Reward: 1292.093, Epsilon: 0.180, Steps: 162, Time: 0.31s\n",
      "Episode 141: Total Reward: 1884.440, Epsilon: 0.158, Steps: 163, Time: 0.33s\n",
      "Episode 151: Total Reward: 1958.221, Epsilon: 0.139, Steps: 159, Time: 0.28s\n",
      "Episode 161: Total Reward: 1597.244, Epsilon: 0.122, Steps: 158, Time: 0.27s\n",
      "Episode 171: Total Reward: 2059.493, Epsilon: 0.107, Steps: 155, Time: 0.26s\n",
      "Episode 181: Total Reward: 1569.876, Epsilon: 0.094, Steps: 161, Time: 0.28s\n",
      "Episode 191: Total Reward: 1582.813, Epsilon: 0.082, Steps: 606, Time: 1.17s\n",
      "Episode 201: Total Reward: 1831.405, Epsilon: 0.072, Steps: 161, Time: 0.32s\n",
      "Checkpoint Reached!\n",
      "Episode 211: Total Reward: 1870.386, Epsilon: 0.063, Steps: 191, Time: 0.33s\n",
      "Episode 221: Total Reward: 2013.313, Epsilon: 0.055, Steps: 152, Time: 0.25s\n",
      "Episode 231: Total Reward: 760.957, Epsilon: 0.049, Steps: 120, Time: 0.19s\n",
      "Episode 241: Total Reward: 1501.441, Epsilon: 0.043, Steps: 149, Time: 0.24s\n",
      "Episode 251: Total Reward: 1686.971, Epsilon: 0.037, Steps: 148, Time: 0.30s\n",
      "Episode 261: Total Reward: 1985.042, Epsilon: 0.033, Steps: 147, Time: 0.29s\n",
      "Episode 271: Total Reward: 1798.241, Epsilon: 0.029, Steps: 147, Time: 0.27s\n",
      "Episode 281: Total Reward: 2262.244, Epsilon: 0.025, Steps: 153, Time: 0.27s\n",
      "Episode 291: Total Reward: 2453.029, Epsilon: 0.022, Steps: 153, Time: 0.25s\n",
      "Episode 301: Total Reward: 1268.169, Epsilon: 0.019, Steps: 148, Time: 0.27s\n",
      "Checkpoint Reached!\n",
      "Episode 311: Total Reward: 2397.056, Epsilon: 0.017, Steps: 155, Time: 0.26s\n",
      "Episode 321: Total Reward: 2529.021, Epsilon: 0.015, Steps: 156, Time: 0.28s\n",
      "Episode 331: Total Reward: 1993.537, Epsilon: 0.013, Steps: 152, Time: 0.27s\n",
      "Episode 341: Total Reward: 2002.994, Epsilon: 0.012, Steps: 155, Time: 0.26s\n",
      "Episode 351: Total Reward: 2120.557, Epsilon: 0.010, Steps: 152, Time: 0.25s\n",
      "Episode 361: Total Reward: 1937.085, Epsilon: 0.009, Steps: 152, Time: 0.25s\n",
      "Episode 371: Total Reward: 1155.628, Epsilon: 0.008, Steps: 128, Time: 0.22s\n",
      "Episode 381: Total Reward: 878.391, Epsilon: 0.007, Steps: 121, Time: 0.20s\n",
      "Episode 391: Total Reward: 681.188, Epsilon: 0.006, Steps: 440, Time: 0.82s\n",
      "Episode 401: Total Reward: 2496.034, Epsilon: 0.005, Steps: 970, Time: 1.83s\n",
      "Checkpoint Reached!\n",
      "Episode 411: Total Reward: 890.731, Epsilon: 0.005, Steps: 122, Time: 0.23s\n",
      "Episode 421: Total Reward: 1442.043, Epsilon: 0.004, Steps: 117, Time: 0.21s\n",
      "Episode 431: Total Reward: 2308.332, Epsilon: 0.004, Steps: 272, Time: 0.48s\n",
      "Episode 441: Total Reward: 1126.727, Epsilon: 0.003, Steps: 173, Time: 0.34s\n",
      "Episode 451: Total Reward: 3144.262, Epsilon: 0.003, Steps: 259, Time: 0.48s\n",
      "Episode 461: Total Reward: 1096.565, Epsilon: 0.002, Steps: 162, Time: 0.30s\n",
      "Episode 471: Total Reward: 3007.064, Epsilon: 0.002, Steps: 227, Time: 0.44s\n",
      "Episode 481: Total Reward: 1750.083, Epsilon: 0.002, Steps: 153, Time: 0.25s\n",
      "Episode 491: Total Reward: 1143.617, Epsilon: 0.002, Steps: 156, Time: 0.24s\n",
      "Episode 501: Total Reward: 2209.067, Epsilon: 0.001, Steps: 155, Time: 0.31s\n",
      "Checkpoint Reached!\n",
      "Episode 511: Total Reward: 1813.634, Epsilon: 0.001, Steps: 172, Time: 0.33s\n",
      "Episode 521: Total Reward: 2524.208, Epsilon: 0.001, Steps: 161, Time: 0.34s\n",
      "Episode 531: Total Reward: 1674.130, Epsilon: 0.001, Steps: 159, Time: 0.32s\n",
      "Episode 541: Total Reward: 1545.136, Epsilon: 0.001, Steps: 157, Time: 0.28s\n",
      "Early stopping condition met! Stopping training.\n",
      "Episode 548: Total Reward: 528.535, Epsilon: 0.001, Steps: 87, Time: 0.66s\n"
     ]
    }
   ],
   "source": [
    "env_name = \"MountainCar-v0\"\n",
    "mountain_env = gym.make(env_name, max_episode_steps=1000, render_mode=\"rgb_array\")\n",
    "\n",
    "trained_agent, rewards_per_episode, epsilon_values, episode_times, steps_per_episode = train_q_values(mountain_env, env_name, training_episodes=2000)\n",
    "checkpoint = {\n",
    "    'policy_net_state_dict': trained_agent.policy_net.state_dict(),\n",
    "    'optimizer_state_dict': trained_agent.optimizer.state_dict(),\n",
    "    'total_steps': trained_agent.total_steps,\n",
    "    'rewards_per_episode': rewards_per_episode,\n",
    "    'epsilon_values': epsilon_values,\n",
    "    'episode_times': episode_times,\n",
    "    'steps_per_episode': steps_per_episode\n",
    "}\n",
    "torch.save(checkpoint, \"dqn_mountain_car_env_v0_checkpoint.pth\")\n",
    "torch.save(trained_agent.policy_net.state_dict(), \"dqn_mountain_car_env_v0.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = -1000.0\n",
      "Episode 2: Total Reward = -161.0\n",
      "Episode 3: Total Reward = -87.0\n",
      "Episode 4: Total Reward = -1000.0\n",
      "Episode 5: Total Reward = -172.0\n",
      "Episode 6: Total Reward = -1000.0\n",
      "Episode 7: Total Reward = -159.0\n",
      "Episode 8: Total Reward = -163.0\n",
      "Episode 9: Total Reward = -158.0\n",
      "Episode 10: Total Reward = -169.0\n",
      "\n",
      "Average Reward over 10 episodes: -406.9\n"
     ]
    }
   ],
   "source": [
    "rewards_arr = []\n",
    "\n",
    "for episode in range(10):\n",
    "    obs, _ = mountain_env.reset()\n",
    "    state = obs\n",
    "    tot_reward = 0\n",
    "    episode_over = False\n",
    "    \n",
    "    while not episode_over:\n",
    "        with torch.no_grad():\n",
    "            action = trained_agent.select_greedy_action(state)\n",
    "        obs, reward, done, truncated, _ = mountain_env.step(action)\n",
    "        episode_over = done or truncated\n",
    "        state = np.array(obs, dtype=np.float32)\n",
    "        tot_reward += reward\n",
    "    rewards_arr.append(tot_reward)\n",
    "    print(f\"Episode {episode + 1}: Total Reward = {tot_reward}\")\n",
    "\n",
    "# Average reward\n",
    "avg_reward = np.mean(rewards_arr)\n",
    "print(f\"\\nAverage Reward over 10 episodes: {avg_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "        state_dim = mountain_env.observation_space.shape[0]\n",
    "        action_dim = mountain_env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = -177.0, Steps taken: 177\n",
      "Episode 2: Total Reward = -163.0, Steps taken: 163\n",
      "Episode 3: Total Reward = -166.0, Steps taken: 166\n",
      "Episode 4: Total Reward = -168.0, Steps taken: 168\n",
      "Episode 5: Total Reward = -158.0, Steps taken: 158\n",
      "Episode 6: Total Reward = -167.0, Steps taken: 167\n",
      "Episode 7: Total Reward = -164.0, Steps taken: 164\n",
      "Episode 8: Total Reward = -1000.0, Steps taken: 1000\n",
      "Episode 9: Total Reward = -165.0, Steps taken: 165\n",
      "Episode 10: Total Reward = -158.0, Steps taken: 158\n",
      "\n",
      "Average Reward over 10 episodes: -300.26666666666665, Average steps taken over 10 episodes: 248.6\n"
     ]
    }
   ],
   "source": [
    "steps_arr = []\n",
    "\n",
    "for episode in range(10):\n",
    "    obs, _ = mountain_env.reset()\n",
    "    state = obs\n",
    "    tot_reward = 0\n",
    "    episode_over = False\n",
    "    steps_taken = 0\n",
    "    \n",
    "    while not episode_over:\n",
    "        with torch.no_grad():\n",
    "            action = trained_agent.select_greedy_action(state)\n",
    "        obs, reward, done, truncated, _ = mountain_env.step(action)\n",
    "        episode_over = done or truncated\n",
    "        state = np.array(obs, dtype=np.float32)\n",
    "        steps_taken +=1\n",
    "        tot_reward += reward\n",
    "    rewards_arr.append(tot_reward)\n",
    "    steps_arr.append(steps_taken)\n",
    "    print(f\"Episode {episode + 1}: Total Reward = {tot_reward}, Steps taken: {steps_taken}\")\n",
    "\n",
    "# Average reward\n",
    "avg_reward = np.mean(rewards_arr)\n",
    "print(f\"\\nAverage Reward over 10 episodes: {avg_reward}, Average steps taken over 10 episodes: {np.mean(steps_arr)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
