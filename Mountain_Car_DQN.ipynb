{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFM11_y6mH_u"
   },
   "source": [
    "# Mountain Car DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n_e4ChUFmPY6"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "33NAxeazlaAS",
    "outputId": "82e81b47-455b-426b-9eb8-6a146135039a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: swig in c:\\users\\viswa\\anaconda3\\lib\\site-packages (4.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium[box2d] in c:\\users\\viswa\\anaconda3\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\viswa\\anaconda3\\lib\\site-packages (from gymnasium[box2d]) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\viswa\\anaconda3\\lib\\site-packages (from gymnasium[box2d]) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\viswa\\anaconda3\\lib\\site-packages (from gymnasium[box2d]) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\viswa\\anaconda3\\lib\\site-packages (from gymnasium[box2d]) (0.0.4)\n",
      "Requirement already satisfied: box2d-py==2.3.5 in c:\\users\\viswa\\anaconda3\\lib\\site-packages (from gymnasium[box2d]) (2.3.5)\n",
      "Requirement already satisfied: pygame>=2.1.3 in c:\\users\\viswa\\anaconda3\\lib\\site-packages (from gymnasium[box2d]) (2.6.1)\n",
      "Requirement already satisfied: swig==4.* in c:\\users\\viswa\\anaconda3\\lib\\site-packages (from gymnasium[box2d]) (4.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from abc import ABC, abstractmethod\n",
    "from enum import Enum\n",
    "from gymnasium import spaces\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "%pip install swig\n",
    "%pip install \"gymnasium[box2d]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cPZkGCWNmbmf"
   },
   "source": [
    "## Class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "jjbfpeeImeIr"
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 12)\n",
    "        self.fc2 = nn.Linear(12, 8)\n",
    "        self.fc3 = nn.Linear(8, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        return self.fc3(x)\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# DQN Agent\n",
    "class DQNAgent:\n",
    "    BATCH_SIZE = 32\n",
    "    EPSILON_START = 1\n",
    "    EPSILON_CUTOFF = 0.00\n",
    "    EPSILON_DECAY = 0.987\n",
    "    GAMMA = 0.9\n",
    "    LR = 0.01\n",
    "    MEMORY_SIZE = 100000\n",
    "\n",
    "    def __init__(self, env):\n",
    "        state_dim = env.observation_space.shape[0]\n",
    "        action_dim = env.action_space.n\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.policy_net = DQN(state_dim, action_dim).to(self.device)\n",
    "        self.target_net = DQN(state_dim, action_dim).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.LR)\n",
    "        self.memory = ReplayBuffer(self.MEMORY_SIZE)\n",
    "        self.steps_done = 0\n",
    "        self.epsilon = self.EPSILON_START\n",
    "        self.total_steps = 0\n",
    "\n",
    "    def select_e_greedy_action(self, env, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, env.action_space.n - 1)\n",
    "        else:\n",
    "            state = torch.tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                return self.policy_net(state).argmax(dim=1).item()\n",
    "\n",
    "    def select_greedy_action(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            return self.policy_net(state).argmax(dim=1).item()\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.total_steps += 1\n",
    "        self.epsilon = max(self.EPSILON_CUTOFF, self.epsilon * self.EPSILON_DECAY)\n",
    "\n",
    "    def step(self):\n",
    "        if len(self.memory) < self.BATCH_SIZE:\n",
    "            return\n",
    "        batch = self.memory.sample(self.BATCH_SIZE)\n",
    "\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        states = torch.tensor(np.array(states), dtype=torch.float32, device=self.device)\n",
    "        actions = torch.tensor(np.array(actions), dtype=torch.int64, device=self.device).unsqueeze(1)\n",
    "        rewards = torch.tensor(np.array(rewards), dtype=torch.float32, device=self.device).unsqueeze(1)\n",
    "        next_states = torch.tensor(np.array(next_states), dtype=torch.float32, device=self.device)\n",
    "        dones = torch.tensor(np.array(dones), dtype=torch.float32, device=self.device).unsqueeze(1)\n",
    "        current_q = self.policy_net(states).gather(1, actions)\n",
    "        next_q = self.target_net(next_states).max(1, keepdim=True)[0].detach()\n",
    "        target_q = rewards + (self.GAMMA * next_q * (1 - dones))\n",
    "\n",
    "        loss = F.mse_loss(target_q, current_q)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tk5hDKlcm0FW"
   },
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "M7xxQtzQm2AN"
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_q_values(env, env_name, target_update_interval=10000, training_episodes=1000, agent_class=DQNAgent):\n",
    "\n",
    "    SPLITS = 30\n",
    "    OBS_LOW_VALUES = env.observation_space.low\n",
    "    OBS_HIGH_VALUES = env.observation_space.high\n",
    "    \n",
    "    POS_VALUES =  np.linspace(OBS_LOW_VALUES[0], OBS_HIGH_VALUES[0], SPLITS)\n",
    "    VEL_VALUES = np.linspace(OBS_LOW_VALUES[1], OBS_HIGH_VALUES[1], SPLITS) \n",
    "    \n",
    "    def quantize_state(state):\n",
    "        return torch.FloatTensor([\n",
    "            np.digitize(state[0], POS_VALUES), \n",
    "            np.digitize(state[1], VEL_VALUES)\n",
    "        ])\n",
    "    \n",
    "    agent = agent_class(env)\n",
    "    rewards_per_episode = []\n",
    "    epsilon_values = []\n",
    "    episode_times = []\n",
    "    start_episode = 0\n",
    "    early_stop_counter = 0\n",
    "\n",
    "    for episode in range(start_episode, training_episodes):\n",
    "        start_time = time.time()\n",
    "        obs, _ = env.reset()\n",
    "        state = quantize_state(obs)\n",
    "        total_reward = 0\n",
    "        episode_over = False\n",
    "        steps_taken = 0\n",
    "        episode_transitions = []\n",
    "\n",
    "        epsilon_values.append(agent.epsilon)\n",
    "\n",
    "        while not episode_over:\n",
    "            action = agent.select_e_greedy_action(env, state)\n",
    "            obs, reward, done, truncated, _ = env.step(action)\n",
    "            reward += 2 * abs(state[1])\n",
    "            reward += 10000 if state[0] >= np.max(OBS_HIGH_VALUES) else 0\n",
    "            next_state = quantize_state(np.array(obs, dtype=np.float32))\n",
    "            agent.memory.push(state, action, reward, next_state, done)\n",
    "            episode_over = done or truncated\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            steps_taken += 1\n",
    "            \n",
    "            agent.step()\n",
    "            \n",
    "            if agent.total_steps % target_update_interval == 0:\n",
    "                agent.update_target_network()\n",
    "    \n",
    "        agent.decay_epsilon()\n",
    "        rewards_per_episode.append(total_reward)\n",
    "        end_time = time.time()\n",
    "        episode_time = end_time - start_time\n",
    "        episode_times.append(episode_time)\n",
    "\n",
    "        if episode % 10 == 0:\n",
    "            print(f\"Episode {episode + 1}: Total Reward: {total_reward}, Epsilon: {agent.epsilon:.3f}, Steps: {steps_taken}, Time: {episode_time:.2f}s\")\n",
    "        if episode % 100 == 0:\n",
    "            # Saving model\n",
    "            torch.save(agent.policy_net.state_dict(), \"dqn_mountain_car_env_v0.pth\")\n",
    "            # Creating checkpoint dictionary\n",
    "            checkpoint = {\n",
    "                'policy_net_state_dict': agent.policy_net.state_dict(),\n",
    "                'optimizer_state_dict': agent.optimizer.state_dict(),\n",
    "                'total_steps': agent.total_steps,\n",
    "                'current_episode': episode,\n",
    "                'rewards_per_episode': rewards_per_episode,\n",
    "                'epsilon_values': epsilon_values,\n",
    "                'episode_times': episode_times\n",
    "            }\n",
    "            torch.save(checkpoint, \"dqn_mountain_car_checkpoints.pth\")\n",
    "            print(\"Checkpoint Reached!\")\n",
    "\n",
    "    return agent, rewards_per_episode, epsilon_values, episode_times\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uv3APuaVmTuD"
   },
   "source": [
    "## Mountain Car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sXrtdzxtmFRc",
    "outputId": "b57e2d07-44e9-4dd5-87e3-99f9577d1227",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward: 10028998.0, Epsilon: 0.987, Steps: 1000, Time: 2.39s\n",
      "Checkpoint Reached!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\viswa\\AppData\\Local\\Temp\\ipykernel_25592\\2665844844.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state = torch.tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 11: Total Reward: 10028898.0, Epsilon: 0.866, Steps: 1000, Time: 2.35s\n",
      "Episode 21: Total Reward: 10028878.0, Epsilon: 0.760, Steps: 1000, Time: 2.35s\n",
      "Episode 31: Total Reward: 10029162.0, Epsilon: 0.667, Steps: 1000, Time: 2.42s\n",
      "Episode 41: Total Reward: 10028998.0, Epsilon: 0.585, Steps: 1000, Time: 4.01s\n",
      "Episode 51: Total Reward: 10028924.0, Epsilon: 0.513, Steps: 1000, Time: 2.98s\n",
      "Episode 61: Total Reward: 10029044.0, Epsilon: 0.450, Steps: 1000, Time: 3.08s\n",
      "Episode 71: Total Reward: 10028932.0, Epsilon: 0.395, Steps: 1000, Time: 2.77s\n",
      "Episode 81: Total Reward: 10029082.0, Epsilon: 0.346, Steps: 1000, Time: 2.81s\n",
      "Episode 91: Total Reward: 10029106.0, Epsilon: 0.304, Steps: 1000, Time: 2.68s\n",
      "Episode 101: Total Reward: 10029072.0, Epsilon: 0.267, Steps: 1000, Time: 3.54s\n",
      "Checkpoint Reached!\n",
      "Episode 111: Total Reward: 10029240.0, Epsilon: 0.234, Steps: 1000, Time: 2.86s\n",
      "Episode 121: Total Reward: 10029092.0, Epsilon: 0.205, Steps: 1000, Time: 3.00s\n",
      "Episode 131: Total Reward: 10028964.0, Epsilon: 0.180, Steps: 1000, Time: 2.93s\n",
      "Episode 141: Total Reward: 10028970.0, Epsilon: 0.158, Steps: 1000, Time: 3.99s\n",
      "Episode 151: Total Reward: 10029054.0, Epsilon: 0.139, Steps: 1000, Time: 3.25s\n",
      "Episode 161: Total Reward: 10029070.0, Epsilon: 0.122, Steps: 1000, Time: 3.28s\n",
      "Episode 171: Total Reward: 10028986.0, Epsilon: 0.107, Steps: 1000, Time: 3.27s\n",
      "Episode 181: Total Reward: 10028910.0, Epsilon: 0.094, Steps: 1000, Time: 2.82s\n",
      "Episode 191: Total Reward: 10029008.0, Epsilon: 0.082, Steps: 1000, Time: 3.35s\n",
      "Episode 201: Total Reward: 10028932.0, Epsilon: 0.072, Steps: 1000, Time: 3.00s\n",
      "Checkpoint Reached!\n"
     ]
    }
   ],
   "source": [
    "env_name = \"MountainCar-v0\"\n",
    "mountain_env = gym.make(env_name, max_episode_steps=1000, render_mode=\"rgb_array\")\n",
    "\n",
    "trained_agent, rewards_per_episode, epsilon_values, episode_times = train_q_values(mountain_env, env_name, training_episodes=20000)\n",
    "torch.save(trained_agent.policy_net.state_dict(), \"dqn_mountain_car_env_v0.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
