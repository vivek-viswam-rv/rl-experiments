{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFM11_y6mH_u"
   },
   "source": [
    "# Mountain Car DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n_e4ChUFmPY6"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "33NAxeazlaAS",
    "outputId": "82e81b47-455b-426b-9eb8-6a146135039a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: swig in c:\\users\\viswa\\anaconda3\\lib\\site-packages (4.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium[box2d] in c:\\users\\viswa\\anaconda3\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\viswa\\anaconda3\\lib\\site-packages (from gymnasium[box2d]) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\viswa\\anaconda3\\lib\\site-packages (from gymnasium[box2d]) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\viswa\\anaconda3\\lib\\site-packages (from gymnasium[box2d]) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\viswa\\anaconda3\\lib\\site-packages (from gymnasium[box2d]) (0.0.4)\n",
      "Requirement already satisfied: box2d-py==2.3.5 in c:\\users\\viswa\\anaconda3\\lib\\site-packages (from gymnasium[box2d]) (2.3.5)\n",
      "Requirement already satisfied: pygame>=2.1.3 in c:\\users\\viswa\\anaconda3\\lib\\site-packages (from gymnasium[box2d]) (2.6.1)\n",
      "Requirement already satisfied: swig==4.* in c:\\users\\viswa\\anaconda3\\lib\\site-packages (from gymnasium[box2d]) (4.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from abc import ABC, abstractmethod\n",
    "from enum import Enum\n",
    "from gymnasium import spaces\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "%pip install swig\n",
    "%pip install \"gymnasium[box2d]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cPZkGCWNmbmf"
   },
   "source": [
    "## Class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "jjbfpeeImeIr"
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 12)\n",
    "        self.fc2 = nn.Linear(12, 8)\n",
    "        self.fc3 = nn.Linear(8, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        return self.fc3(x)\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# DQN Agent\n",
    "class DQNAgent:\n",
    "    BATCH_SIZE = 32\n",
    "    EPSILON_START = 1\n",
    "    EPSILON_CUTOFF = 0.005\n",
    "    EPSILON_DECAY = 0.987\n",
    "    GAMMA = 0.9\n",
    "    LR = 75e-5\n",
    "    MEMORY_SIZE = 50000\n",
    "\n",
    "    def __init__(self, env):\n",
    "        state_dim = env.observation_space.shape[0]\n",
    "        action_dim = env.action_space.n\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.policy_net = DQN(state_dim, action_dim).to(self.device)\n",
    "        self.target_net = DQN(state_dim, action_dim).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.LR)\n",
    "        self.memory = ReplayBuffer(self.MEMORY_SIZE)\n",
    "        self.steps_done = 0\n",
    "        self.epsilon = self.EPSILON_START\n",
    "        self.total_steps = 0\n",
    "\n",
    "    def select_e_greedy_action(self, env, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, env.action_space.n - 1)\n",
    "        else:\n",
    "            state = torch.tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                return self.policy_net(state).argmax(dim=1).item()\n",
    "\n",
    "    def select_greedy_action(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            return self.policy_net(state).argmax(dim=1).item()\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.total_steps += 1\n",
    "        self.epsilon = max(self.EPSILON_CUTOFF, self.epsilon * self.EPSILON_DECAY)\n",
    "\n",
    "    def step(self):\n",
    "        if len(self.memory) < self.BATCH_SIZE:\n",
    "            return\n",
    "        batch = self.memory.sample(self.BATCH_SIZE)\n",
    "\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        states = torch.tensor(np.array(states), dtype=torch.float32, device=self.device)\n",
    "        actions = torch.tensor(np.array(actions), dtype=torch.int64, device=self.device).unsqueeze(1)\n",
    "        rewards = torch.tensor(np.array(rewards), dtype=torch.float32, device=self.device).unsqueeze(1)\n",
    "        next_states = torch.tensor(np.array(next_states), dtype=torch.float32, device=self.device)\n",
    "        dones = torch.tensor(np.array(dones), dtype=torch.float32, device=self.device).unsqueeze(1)\n",
    "        current_q = self.policy_net(states).gather(1, actions)\n",
    "        next_q = self.target_net(next_states).max(1, keepdim=True)[0].detach()\n",
    "        target_q = rewards + (self.GAMMA * next_q * (1 - dones))\n",
    "\n",
    "        loss = F.mse_loss(target_q, current_q)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tk5hDKlcm0FW"
   },
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "M7xxQtzQm2AN"
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_q_values(env, env_name, target_update_interval=2000, training_episodes=1000, agent_class=DQNAgent):\n",
    "    OBS_HIGH_VALUES = env.observation_space.high\n",
    "    \n",
    "    agent = agent_class(env)\n",
    "    rewards_per_episode = []\n",
    "    steps_per_episode = []\n",
    "    epsilon_values = []\n",
    "    episode_times = []\n",
    "    start_episode = 0\n",
    "    early_stop_counter = 0\n",
    "\n",
    "    for episode in range(start_episode, training_episodes):\n",
    "        start_time = time.time()\n",
    "        obs, _ = env.reset()\n",
    "        state = obs\n",
    "        total_reward = 0\n",
    "        episode_over = False\n",
    "        steps_taken = 0\n",
    "        episode_transitions = []\n",
    "\n",
    "        epsilon_values.append(agent.epsilon)\n",
    "\n",
    "        while not episode_over:\n",
    "            action = agent.select_e_greedy_action(env, state)\n",
    "            obs, reward, done, truncated, _ = env.step(action)\n",
    "            reward += 3 * abs(state[1])\n",
    "            reward += 100000 if state[0] >= np.max(OBS_HIGH_VALUES[0]) else 0\n",
    "            next_state = np.array(obs, dtype=np.float32)\n",
    "            agent.memory.push(state, action, reward, next_state, done)\n",
    "            episode_over = done or truncated\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            steps_taken += 1\n",
    "            \n",
    "            agent.step()\n",
    "            \n",
    "            if agent.total_steps % target_update_interval == 0:\n",
    "                agent.update_target_network()\n",
    "\n",
    "        if steps_taken <= 110:\n",
    "            print(f\"Episode {episode + 1:}: Total Reward: {total_reward:.3f}, Epsilon: {agent.epsilon:.3f}, Steps: {steps_taken}, Time: {episode_time:.2f}s\")\n",
    "            break\n",
    "        \n",
    "        agent.decay_epsilon()\n",
    "        rewards_per_episode.append(total_reward)\n",
    "        steps_per_episode.append(steps_taken)\n",
    "        end_time = time.time()\n",
    "        episode_time = end_time - start_time\n",
    "        episode_times.append(episode_time)\n",
    "\n",
    "        if episode % 10 == 0:\n",
    "            print(f\"Episode {episode + 1:}: Total Reward: {total_reward:.3f}, Epsilon: {agent.epsilon:.3f}, Steps: {steps_taken}, Time: {episode_time:.2f}s\")\n",
    "        if episode % 100 == 0:\n",
    "            # Saving model\n",
    "            torch.save(agent.policy_net.state_dict(), \"dqn_mountain_car_env_v0.pth\")\n",
    "            checkpoint = {\n",
    "                'policy_net_state_dict': agent.policy_net.state_dict(),\n",
    "                'optimizer_state_dict': agent.optimizer.state_dict(),\n",
    "                'steps_done': agent.steps_done,\n",
    "                'episode': episode,\n",
    "                'rewards': rewards_per_episode,\n",
    "                'epsilons': epsilon_values,\n",
    "                'times': episode_times\n",
    "            }\n",
    "            torch.save(checkpoint, \"dqn_mountain_car_env_v0_checkpoint.pth\")\n",
    "            print(\"Checkpoint Reached!\")\n",
    "\n",
    "    return agent, rewards_per_episode, epsilon_values, episode_times, steps_per_episode\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uv3APuaVmTuD"
   },
   "source": [
    "## Mountain Car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sXrtdzxtmFRc",
    "outputId": "b57e2d07-44e9-4dd5-87e3-99f9577d1227",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward: -988.678, Epsilon: 0.987, Steps: 1000, Time: 1.50s\n",
      "Checkpoint Reached!\n",
      "Episode 11: Total Reward: -992.208, Epsilon: 0.866, Steps: 1000, Time: 1.46s\n",
      "Episode 21: Total Reward: -981.603, Epsilon: 0.760, Steps: 1000, Time: 1.61s\n",
      "Episode 31: Total Reward: -982.253, Epsilon: 0.667, Steps: 1000, Time: 1.53s\n",
      "Episode 41: Total Reward: -989.893, Epsilon: 0.585, Steps: 1000, Time: 1.51s\n",
      "Episode 51: Total Reward: -990.714, Epsilon: 0.513, Steps: 1000, Time: 1.54s\n",
      "Episode 61: Total Reward: -985.377, Epsilon: 0.450, Steps: 1000, Time: 1.61s\n",
      "Episode 71: Total Reward: -994.694, Epsilon: 0.395, Steps: 1000, Time: 1.65s\n",
      "Episode 81: Total Reward: -993.932, Epsilon: 0.346, Steps: 1000, Time: 1.69s\n",
      "Episode 91: Total Reward: -992.484, Epsilon: 0.304, Steps: 1000, Time: 1.55s\n",
      "Episode 101: Total Reward: -994.288, Epsilon: 0.267, Steps: 1000, Time: 1.76s\n",
      "Checkpoint Reached!\n",
      "Episode 111: Total Reward: -995.980, Epsilon: 0.234, Steps: 1000, Time: 1.61s\n",
      "Episode 121: Total Reward: -991.496, Epsilon: 0.205, Steps: 1000, Time: 1.59s\n",
      "Episode 131: Total Reward: -991.540, Epsilon: 0.180, Steps: 1000, Time: 1.68s\n",
      "Episode 141: Total Reward: -994.203, Epsilon: 0.158, Steps: 1000, Time: 1.70s\n",
      "Episode 151: Total Reward: -990.346, Epsilon: 0.139, Steps: 1000, Time: 1.73s\n",
      "Episode 161: Total Reward: -627.687, Epsilon: 0.122, Steps: 635, Time: 1.09s\n",
      "Episode 171: Total Reward: -624.744, Epsilon: 0.107, Steps: 631, Time: 1.04s\n",
      "Episode 181: Total Reward: -990.080, Epsilon: 0.094, Steps: 1000, Time: 1.73s\n",
      "Episode 191: Total Reward: -987.928, Epsilon: 0.082, Steps: 1000, Time: 1.73s\n",
      "Episode 201: Total Reward: -358.822, Epsilon: 0.072, Steps: 364, Time: 0.57s\n",
      "Checkpoint Reached!\n",
      "Episode 211: Total Reward: -980.779, Epsilon: 0.063, Steps: 1000, Time: 1.59s\n",
      "Episode 221: Total Reward: -981.388, Epsilon: 0.055, Steps: 1000, Time: 1.66s\n",
      "Episode 231: Total Reward: -797.752, Epsilon: 0.049, Steps: 809, Time: 1.33s\n",
      "Episode 241: Total Reward: -247.795, Epsilon: 0.043, Steps: 252, Time: 0.42s\n",
      "Episode 251: Total Reward: -231.880, Epsilon: 0.037, Steps: 236, Time: 0.45s\n",
      "Episode 261: Total Reward: -401.672, Epsilon: 0.033, Steps: 408, Time: 0.69s\n",
      "Episode 271: Total Reward: -645.352, Epsilon: 0.029, Steps: 656, Time: 1.11s\n",
      "Episode 281: Total Reward: -302.298, Epsilon: 0.025, Steps: 308, Time: 0.53s\n",
      "Episode 291: Total Reward: -323.838, Epsilon: 0.022, Steps: 330, Time: 0.53s\n",
      "Episode 301: Total Reward: -381.696, Epsilon: 0.019, Steps: 387, Time: 0.60s\n",
      "Checkpoint Reached!\n",
      "Episode 311: Total Reward: -248.374, Epsilon: 0.017, Steps: 253, Time: 0.45s\n",
      "Episode 321: Total Reward: -654.637, Epsilon: 0.015, Steps: 663, Time: 1.08s\n",
      "Episode 331: Total Reward: -752.316, Epsilon: 0.013, Steps: 765, Time: 1.33s\n",
      "Episode 341: Total Reward: -140.070, Epsilon: 0.012, Steps: 142, Time: 0.27s\n",
      "Episode 351: Total Reward: -332.575, Epsilon: 0.010, Steps: 338, Time: 0.59s\n",
      "Episode 361: Total Reward: -292.077, Epsilon: 0.009, Steps: 298, Time: 0.47s\n",
      "Episode 371: Total Reward: -240.870, Epsilon: 0.008, Steps: 246, Time: 0.42s\n",
      "Episode 381: Total Reward: -322.077, Epsilon: 0.007, Steps: 329, Time: 0.55s\n",
      "Episode 391: Total Reward: -325.359, Epsilon: 0.006, Steps: 331, Time: 0.55s\n",
      "Episode 401: Total Reward: -168.655, Epsilon: 0.005, Steps: 171, Time: 0.29s\n",
      "Checkpoint Reached!\n",
      "Episode 411: Total Reward: -263.289, Epsilon: 0.005, Steps: 267, Time: 0.47s\n",
      "Episode 421: Total Reward: -198.270, Epsilon: 0.005, Steps: 203, Time: 0.40s\n",
      "Episode 431: Total Reward: -219.541, Epsilon: 0.005, Steps: 224, Time: 0.38s\n",
      "Episode 441: Total Reward: -260.474, Epsilon: 0.005, Steps: 266, Time: 0.44s\n",
      "Episode 451: Total Reward: -227.483, Epsilon: 0.005, Steps: 232, Time: 0.44s\n",
      "Episode 461: Total Reward: -232.336, Epsilon: 0.005, Steps: 236, Time: 0.38s\n",
      "Episode 471: Total Reward: -355.509, Epsilon: 0.005, Steps: 362, Time: 0.64s\n",
      "Episode 481: Total Reward: -153.537, Epsilon: 0.005, Steps: 156, Time: 0.27s\n",
      "Episode 491: Total Reward: -229.769, Epsilon: 0.005, Steps: 234, Time: 0.40s\n",
      "Episode 501: Total Reward: -236.035, Epsilon: 0.005, Steps: 241, Time: 0.39s\n",
      "Checkpoint Reached!\n",
      "Episode 511: Total Reward: -213.929, Epsilon: 0.005, Steps: 219, Time: 0.37s\n",
      "Episode 521: Total Reward: -167.403, Epsilon: 0.005, Steps: 170, Time: 0.35s\n",
      "Episode 531: Total Reward: -234.990, Epsilon: 0.005, Steps: 238, Time: 0.38s\n",
      "Episode 541: Total Reward: -336.478, Epsilon: 0.005, Steps: 340, Time: 0.59s\n",
      "Episode 551: Total Reward: -249.869, Epsilon: 0.005, Steps: 255, Time: 0.48s\n",
      "Episode 561: Total Reward: -298.254, Epsilon: 0.005, Steps: 304, Time: 0.52s\n",
      "Episode 571: Total Reward: -134.624, Epsilon: 0.005, Steps: 137, Time: 0.22s\n",
      "Episode 581: Total Reward: -219.660, Epsilon: 0.005, Steps: 224, Time: 0.37s\n",
      "Episode 591: Total Reward: -153.660, Epsilon: 0.005, Steps: 156, Time: 0.27s\n",
      "Episode 601: Total Reward: -289.230, Epsilon: 0.005, Steps: 295, Time: 0.48s\n",
      "Checkpoint Reached!\n",
      "Episode 611: Total Reward: -206.212, Epsilon: 0.005, Steps: 210, Time: 0.40s\n",
      "Episode 621: Total Reward: -171.030, Epsilon: 0.005, Steps: 174, Time: 0.31s\n",
      "Episode 631: Total Reward: -206.958, Epsilon: 0.005, Steps: 209, Time: 0.34s\n",
      "Episode 641: Total Reward: -210.515, Epsilon: 0.005, Steps: 214, Time: 0.35s\n",
      "Episode 651: Total Reward: -270.618, Epsilon: 0.005, Steps: 274, Time: 0.47s\n",
      "Episode 661: Total Reward: -159.748, Epsilon: 0.005, Steps: 164, Time: 0.26s\n",
      "Episode 671: Total Reward: -219.391, Epsilon: 0.005, Steps: 224, Time: 0.40s\n",
      "Episode 681: Total Reward: -143.441, Epsilon: 0.005, Steps: 146, Time: 0.24s\n",
      "Episode 691: Total Reward: -161.823, Epsilon: 0.005, Steps: 165, Time: 0.34s\n",
      "Episode 701: Total Reward: -221.622, Epsilon: 0.005, Steps: 225, Time: 0.38s\n",
      "Checkpoint Reached!\n",
      "Episode 711: Total Reward: -203.656, Epsilon: 0.005, Steps: 208, Time: 0.42s\n",
      "Episode 721: Total Reward: -121.242, Epsilon: 0.005, Steps: 124, Time: 0.20s\n",
      "Episode 731: Total Reward: -189.702, Epsilon: 0.005, Steps: 194, Time: 0.32s\n",
      "Episode 741: Total Reward: -160.870, Epsilon: 0.005, Steps: 164, Time: 0.29s\n",
      "Episode 751: Total Reward: -147.566, Epsilon: 0.005, Steps: 150, Time: 0.24s\n",
      "Episode 761: Total Reward: -135.338, Epsilon: 0.005, Steps: 138, Time: 0.26s\n",
      "Episode 771: Total Reward: -133.598, Epsilon: 0.005, Steps: 136, Time: 0.28s\n",
      "Episode 781: Total Reward: -228.417, Epsilon: 0.005, Steps: 233, Time: 0.40s\n",
      "Episode 791: Total Reward: -216.706, Epsilon: 0.005, Steps: 222, Time: 0.36s\n",
      "Episode 801: Total Reward: -204.319, Epsilon: 0.005, Steps: 209, Time: 0.40s\n",
      "Checkpoint Reached!\n",
      "Episode 811: Total Reward: -159.294, Epsilon: 0.005, Steps: 163, Time: 0.27s\n",
      "Episode 821: Total Reward: -213.973, Epsilon: 0.005, Steps: 219, Time: 0.35s\n",
      "Episode 831: Total Reward: -209.368, Epsilon: 0.005, Steps: 214, Time: 0.37s\n",
      "Episode 841: Total Reward: -164.982, Epsilon: 0.005, Steps: 168, Time: 0.27s\n",
      "Episode 851: Total Reward: -187.139, Epsilon: 0.005, Steps: 190, Time: 0.36s\n",
      "Episode 861: Total Reward: -212.309, Epsilon: 0.005, Steps: 217, Time: 0.39s\n",
      "Episode 871: Total Reward: -121.231, Epsilon: 0.005, Steps: 124, Time: 0.22s\n",
      "Episode 881: Total Reward: -156.723, Epsilon: 0.005, Steps: 160, Time: 0.25s\n",
      "Episode 891: Total Reward: -197.610, Epsilon: 0.005, Steps: 202, Time: 0.40s\n",
      "Episode 901: Total Reward: -242.397, Epsilon: 0.005, Steps: 246, Time: 0.40s\n",
      "Checkpoint Reached!\n",
      "Episode 911: Total Reward: -127.372, Epsilon: 0.005, Steps: 130, Time: 0.22s\n",
      "Episode 921: Total Reward: -209.266, Epsilon: 0.005, Steps: 214, Time: 0.38s\n",
      "Episode 931: Total Reward: -159.446, Epsilon: 0.005, Steps: 163, Time: 0.27s\n",
      "Episode 941: Total Reward: -211.401, Epsilon: 0.005, Steps: 216, Time: 0.35s\n",
      "Episode 951: Total Reward: -168.136, Epsilon: 0.005, Steps: 171, Time: 0.31s\n",
      "Episode 961: Total Reward: -206.384, Epsilon: 0.005, Steps: 211, Time: 0.43s\n",
      "Episode 971: Total Reward: -187.678, Epsilon: 0.005, Steps: 192, Time: 0.33s\n",
      "Episode 981: Total Reward: -216.625, Epsilon: 0.005, Steps: 221, Time: 0.43s\n",
      "Episode 991: Total Reward: -242.017, Epsilon: 0.005, Steps: 247, Time: 0.41s\n"
     ]
    }
   ],
   "source": [
    "env_name = \"MountainCar-v0\"\n",
    "mountain_env = gym.make(env_name, max_episode_steps=1000, render_mode=\"rgb_array\")\n",
    "\n",
    "trained_agent, rewards_per_episode, epsilon_values, episode_times = train_q_values(mountain_env, env_name, training_episodes=1000)\n",
    "torch.save(trained_agent.policy_net.state_dict(), \"dqn_mountain_car_env_v0.pth\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
