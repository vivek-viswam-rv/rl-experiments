{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XQDzpaYp0Bxm"
   },
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "^C\n",
      "^CRequirement already satisfied: gymnasium[atari] in c:\\programdata\\anaconda3\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\users\\sunny_aiv3tl7\\appdata\\roaming\\python\\python38\\site-packages (from gymnasium[atari]) (4.11.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gymnasium[atari]) (1.6.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\sunny_aiv3tl7\\appdata\\roaming\\python\\python38\\site-packages (from gymnasium[atari]) (0.0.4)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\sunny_aiv3tl7\\appdata\\roaming\\python\\python38\\site-packages (from gymnasium[atari]) (4.12.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\sunny_aiv3tl7\\appdata\\roaming\\python\\python38\\site-packages (from gymnasium[atari]) (1.22.0)\n",
      "Requirement already satisfied: ale-py>=0.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from gymnasium[atari]) (0.10.1)\n",
      "Requirement already satisfied: importlib-resources in c:\\programdata\\anaconda3\\lib\\site-packages (from ale-py>=0.9->gymnasium[atari]) (6.4.5)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.8.0->gymnasium[atari]) (3.4.1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Install\n",
    "!pip install gymnasium[atari]\n",
    "!pip install autorom[accept-rom-license]\n",
    "!pip install torch\n",
    "\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Imports\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import AtariPreprocessing, FrameStackObservation\n",
    "import matplotlib.pyplot as plt\n",
    "import ale_py\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Register\n",
    "gym.register_envs(ale_py)\n",
    "\n",
    "# DQN\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64 * 7 * 7, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 6)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# DQN Agent\n",
    "class DQNAgent:\n",
    "    BATCH_SIZE = 32\n",
    "    EPSILON_START = 1\n",
    "    EPSILON_CUTOFF = 0.05\n",
    "    EPSILON_DECAY = 0.9977 # Will take about 1000 episodes to reach 0.1 and 1300 to reach 0.05\n",
    "    GAMMA = 0.99\n",
    "    LR = 1e-4\n",
    "    MEMORY_SIZE = 30000\n",
    "    total_steps = 0\n",
    "\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.policy_net = DQN().to(self.device)\n",
    "        self.target_net = DQN().to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.LR)\n",
    "        self.memory = ReplayBuffer(self.MEMORY_SIZE)\n",
    "        self.steps_done = 0\n",
    "        self.epsilon = self.EPSILON_START\n",
    "        self.current_episode = 0\n",
    "\n",
    "    def select_e_greedy_action(self, env, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, env.action_space.n - 1)\n",
    "        else:\n",
    "            state = torch.tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                return self.policy_net(state).argmax(dim=1).item()\n",
    "\n",
    "    def select_greedy_action(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            return self.policy_net(state).argmax(dim=1).item()\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.total_steps += 1\n",
    "        self.epsilon = max(self.EPSILON_CUTOFF, self.EPSILON_START * (self.EPSILON_DECAY ** self.current_episode))\n",
    "\n",
    "    def step(self):\n",
    "        if len(self.memory) < self.BATCH_SIZE:\n",
    "            return\n",
    "        batch = self.memory.sample(self.BATCH_SIZE)\n",
    "\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        states = torch.tensor(np.array(states), dtype=torch.float32, device=self.device)\n",
    "        actions = torch.tensor(np.array(actions), dtype=torch.int64, device=self.device).unsqueeze(1)\n",
    "        rewards = torch.tensor(np.array(rewards), dtype=torch.float32, device=self.device).unsqueeze(1)\n",
    "        next_states = torch.tensor(np.array(next_states), dtype=torch.float32, device=self.device)\n",
    "        dones = torch.tensor(np.array(dones), dtype=torch.float32, device=self.device).unsqueeze(1)\n",
    "        current_q = self.policy_net(states).gather(1, actions)\n",
    "        next_q = self.target_net(next_states).max(1, keepdim=True)[0].detach()\n",
    "        target_q = rewards + (self.GAMMA * next_q * (1 - dones))\n",
    "\n",
    "        loss = F.mse_loss(target_q, current_q)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        \n",
    "def train_q_values(env, env_name, target_update_interval=10, training_episodes=1000, agent_class=DQNAgent, checkpoint_path=None):\n",
    "    agent = agent_class()\n",
    "    rewards_per_episode = []\n",
    "    epsilon_values = []\n",
    "    episode_times = []\n",
    "    start_episode = 0\n",
    "    early_stop_counter = 0\n",
    "\n",
    "    # Loading from last checkpoint in case training fails\n",
    "    drive_path = '/content/drive/MyDrive/dqn_pong_checkpoints/'\n",
    "    if checkpoint_path is not None:\n",
    "        checkpoint = torch.load(os.path.join(drive_path, checkpoint_path))\n",
    "        agent.policy_net.load_state_dict(checkpoint['policy_net_state_dict'])\n",
    "        agent.target_net.load_state_dict(checkpoint['policy_net_state_dict'])\n",
    "        agent.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        agent.total_steps = checkpoint['total_steps']\n",
    "        agent.current_episode = checkpoint['current_episode']\n",
    "        rewards_per_episode = checkpoint['rewards_per_episode']\n",
    "        epsilon_values = checkpoint['epsilon_values']\n",
    "        episode_times = checkpoint['episode_times']\n",
    "        start_episode = checkpoint['current_episode'] + 1\n",
    "        agent.epsilon = max(agent.EPSILON_CUTOFF, agent.EPSILON_START * (agent.EPSILON_DECAY ** agent.current_episode))\n",
    "        print(f\"Resumed training from episode {start_episode}\")\n",
    "\n",
    "    for episode in range(start_episode, training_episodes):\n",
    "        agent.current_episode = episode\n",
    "        start_time = time.time()\n",
    "        obs, _ = env.reset()\n",
    "        state = obs\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        steps_taken = 0\n",
    "        episode_transitions = []\n",
    "\n",
    "        epsilon_values.append(agent.epsilon)\n",
    "\n",
    "        while not done:\n",
    "            action = agent.select_e_greedy_action(env, state)\n",
    "            obs, reward, done, _, _ = env.step(action)\n",
    "            next_state = np.array(obs, dtype=np.float32)\n",
    "            episode_transitions.append((state, action, reward, next_state, done))\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            steps_taken += 1\n",
    "\n",
    "            agent.decay_epsilon()\n",
    "\n",
    "        for transition in episode_transitions:\n",
    "            agent.memory.push(*transition)\n",
    "\n",
    "        for _ in range(len(episode_transitions)):\n",
    "            agent.step()\n",
    "\n",
    "        rewards_per_episode.append(total_reward)\n",
    "        end_time = time.time()\n",
    "        episode_time = end_time - start_time\n",
    "        episode_times.append(episode_time)\n",
    "\n",
    "        if total_reward >= 20:\n",
    "            early_stop_counter += 1\n",
    "        else:\n",
    "            early_stop_counter = 0\n",
    "\n",
    "        if early_stop_counter >= 15:\n",
    "            print(f\"Early stopping at episode {episode}\")\n",
    "            torch.save(agent.policy_net.state_dict(), os.path.join(drive_path, \"dqn_pong_env_v5_early_stop_train.pth\"))\n",
    "            break\n",
    "\n",
    "        if episode % 10 == 0:\n",
    "            print(f\"Episode {episode + 1}: Total Reward: {total_reward}, Epsilon: {agent.epsilon:.3f}, Steps: {steps_taken}, Time: {episode_time:.2f}s\")\n",
    "            # Saving model\n",
    "            torch.save(agent.policy_net.state_dict(), os.path.join(drive_path, \"dqn_pong_env_v5.pth\"))\n",
    "            # Creating checkpoint dictionary and saving\n",
    "            checkpoint = {\n",
    "                'policy_net_state_dict': agent.policy_net.state_dict(),\n",
    "                'optimizer_state_dict': agent.optimizer.state_dict(),\n",
    "                'total_steps': agent.total_steps,\n",
    "                'current_episode': agent.current_episode,\n",
    "                'rewards_per_episode': rewards_per_episode,\n",
    "                'epsilon_values': epsilon_values,\n",
    "                'episode_times': episode_times\n",
    "            }\n",
    "            torch.save(checkpoint, os.path.join(drive_path, \"dqn_pong_checkpoints.pth\"))\n",
    "\n",
    "        if episode % target_update_interval == 0:\n",
    "            agent.update_target_network()\n",
    "\n",
    "    return agent, rewards_per_episode, epsilon_values, episode_times\n",
    "\n",
    "env_name = \"PongNoFrameskip-v4\"\n",
    "pong_env = gym.make(env_name, render_mode=\"rgb_array\", frameskip=1)\n",
    "pong_env = AtariPreprocessing(\n",
    "    pong_env,\n",
    "    frame_skip=4,\n",
    "    grayscale_obs=True,\n",
    "    scale_obs=False,\n",
    "    terminal_on_life_loss=False\n",
    ")\n",
    "pong_env = FrameStackObservation(pong_env, stack_size=4)\n",
    "\n",
    "# Creating folder\n",
    "drive_path = '/content/drive/MyDrive/dqn_pong_checkpoints/'\n",
    "os.makedirs(drive_path, exist_ok=True)\n",
    "\n",
    "# Training\n",
    "trained_agent, rewards_per_episode, epsilon_values, episode_times = train_q_values(pong_env, env_name, training_episodes=5000)\n",
    "torch.save(trained_agent.policy_net.state_dict(), os.path.join(drive_path, \"dqn_pongnoframeskip_env_v5.pth\"))\n",
    "\n",
    "# Relevant Plots\n",
    "plt.plot(rewards_per_episode)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(f\"{env_name} Reward Characteristics - Training\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(epsilon_values, label=f\"Decay rate = {DQNAgent.EPSILON_DECAY}\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Epsilon\")\n",
    "plt.title(f\"{env_name} Epsilon Characteristics - Training\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(episode_times)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Time (seconds)\")\n",
    "plt.title(f\"{env_name} Episode Processing Time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fMoJce1azKI9",
    "outputId": "d09e1232-00b8-4356-8db2-49165d5e7c1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium[atari] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[atari]) (2.0.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[atari]) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[atari]) (4.13.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[atari]) (0.0.4)\n",
      "Requirement already satisfied: ale_py>=0.9 in /usr/local/lib/python3.11/dist-packages (from gymnasium[atari]) (0.10.2)\n",
      "Collecting autorom[accept-rom-license]\n",
      "  Downloading AutoROM-0.6.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from autorom[accept-rom-license]) (8.1.8)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from autorom[accept-rom-license]) (2.32.3)\n",
      "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license])\n",
      "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->autorom[accept-rom-license]) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->autorom[accept-rom-license]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->autorom[accept-rom-license]) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->autorom[accept-rom-license]) (2025.1.31)\n",
      "Downloading AutoROM-0.6.1-py3-none-any.whl (9.4 kB)\n",
      "Building wheels for collected packages: AutoROM.accept-rom-license\n",
      "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for AutoROM.accept-rom-license: filename=autorom_accept_rom_license-0.6.1-py3-none-any.whl size=446709 sha256=8ad27fa28155ba78bd88cb2f93a797deb1319fe75689fb38a3326e9704da3a7d\n",
      "  Stored in directory: /root/.cache/pip/wheels/bc/fc/c6/8aa657c0d2089982f2dabd110efc68c61eb49831fdb7397351\n",
      "Successfully built AutoROM.accept-rom-license\n",
      "Installing collected packages: AutoROM.accept-rom-license, autorom\n",
      "Successfully installed AutoROM.accept-rom-license-0.6.1 autorom-0.6.1\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m123.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m101.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m108.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
     ]
    }
   ],
   "source": [
    "# Install\n",
    "!pip install gymnasium[atari]\n",
    "!pip install autorom[accept-rom-license]\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pDvd5QT-zRo0",
    "outputId": "bf6ea2cb-1ae2-4358-f359-17d157d68403"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WIwMct6azVVc"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import AtariPreprocessing, FrameStackObservation\n",
    "import matplotlib.pyplot as plt\n",
    "import ale_py\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zme46j01zXoE"
   },
   "outputs": [],
   "source": [
    "# Register\n",
    "gym.register_envs(ale_py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5uds8Ho50TFk"
   },
   "source": [
    "Defining Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4QJteQOd0OU0"
   },
   "outputs": [],
   "source": [
    "# DQN\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64 * 7 * 7, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 6)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# DQN Agent\n",
    "class DQNAgent:\n",
    "    BATCH_SIZE = 32\n",
    "    EPSILON_START = 1\n",
    "    EPSILON_CUTOFF = 0.05\n",
    "    EPSILON_DECAY = 0.9977 # Will take about 1000 episodes to reach 0.1 and 1300 to reach 0.05\n",
    "    GAMMA = 0.99\n",
    "    LR = 1e-4\n",
    "    MEMORY_SIZE = 30000\n",
    "    total_steps = 0\n",
    "\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.policy_net = DQN().to(self.device)\n",
    "        self.target_net = DQN().to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.LR)\n",
    "        self.memory = ReplayBuffer(self.MEMORY_SIZE)\n",
    "        self.steps_done = 0\n",
    "        self.epsilon = self.EPSILON_START\n",
    "        self.current_episode = 0\n",
    "\n",
    "    def select_e_greedy_action(self, env, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, env.action_space.n - 1)\n",
    "        else:\n",
    "            state = torch.tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                return self.policy_net(state).argmax(dim=1).item()\n",
    "\n",
    "    def select_greedy_action(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            return self.policy_net(state).argmax(dim=1).item()\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.total_steps += 1\n",
    "        self.epsilon = max(self.EPSILON_CUTOFF, self.EPSILON_START * (self.EPSILON_DECAY ** self.current_episode))\n",
    "\n",
    "    def step(self):\n",
    "        if len(self.memory) < self.BATCH_SIZE:\n",
    "            return\n",
    "        batch = self.memory.sample(self.BATCH_SIZE)\n",
    "\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        states = torch.tensor(np.array(states), dtype=torch.float32, device=self.device)\n",
    "        actions = torch.tensor(np.array(actions), dtype=torch.int64, device=self.device).unsqueeze(1)\n",
    "        rewards = torch.tensor(np.array(rewards), dtype=torch.float32, device=self.device).unsqueeze(1)\n",
    "        next_states = torch.tensor(np.array(next_states), dtype=torch.float32, device=self.device)\n",
    "        dones = torch.tensor(np.array(dones), dtype=torch.float32, device=self.device).unsqueeze(1)\n",
    "        current_q = self.policy_net(states).gather(1, actions)\n",
    "        next_q = self.target_net(next_states).max(1, keepdim=True)[0].detach()\n",
    "        target_q = rewards + (self.GAMMA * next_q * (1 - dones))\n",
    "\n",
    "        loss = F.mse_loss(target_q, current_q)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6XCd4I4k0at0"
   },
   "source": [
    "Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u0DUDN-qzDw5"
   },
   "outputs": [],
   "source": [
    "def train_q_values(env, env_name, target_update_interval=10, training_episodes=1000, agent_class=DQNAgent, checkpoint_path=None):\n",
    "    agent = agent_class()\n",
    "    rewards_per_episode = []\n",
    "    epsilon_values = []\n",
    "    episode_times = []\n",
    "    start_episode = 0\n",
    "    early_stop_counter = 0\n",
    "\n",
    "    # Loading from last checkpoint in case training fails\n",
    "    drive_path = '/content/drive/MyDrive/dqn_pong_checkpoints/'\n",
    "    if checkpoint_path is not None:\n",
    "        checkpoint = torch.load(os.path.join(drive_path, checkpoint_path))\n",
    "        agent.policy_net.load_state_dict(checkpoint['policy_net_state_dict'])\n",
    "        agent.target_net.load_state_dict(checkpoint['policy_net_state_dict'])\n",
    "        agent.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        agent.total_steps = checkpoint['total_steps']\n",
    "        agent.current_episode = checkpoint['current_episode']\n",
    "        rewards_per_episode = checkpoint['rewards_per_episode']\n",
    "        epsilon_values = checkpoint['epsilon_values']\n",
    "        episode_times = checkpoint['episode_times']\n",
    "        start_episode = checkpoint['current_episode'] + 1\n",
    "        agent.epsilon = max(agent.EPSILON_CUTOFF, agent.EPSILON_START * (agent.EPSILON_DECAY ** agent.current_episode))\n",
    "        print(f\"Resumed training from episode {start_episode}\")\n",
    "\n",
    "    for episode in range(start_episode, training_episodes):\n",
    "        agent.current_episode = episode\n",
    "        start_time = time.time()\n",
    "        obs, _ = env.reset()\n",
    "        state = obs\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        steps_taken = 0\n",
    "        episode_transitions = []\n",
    "\n",
    "        epsilon_values.append(agent.epsilon)\n",
    "\n",
    "        while not done:\n",
    "            action = agent.select_e_greedy_action(env, state)\n",
    "            obs, reward, done, _, _ = env.step(action)\n",
    "            next_state = np.array(obs, dtype=np.float32)\n",
    "            episode_transitions.append((state, action, reward, next_state, done))\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            steps_taken += 1\n",
    "\n",
    "            agent.decay_epsilon()\n",
    "\n",
    "        for transition in episode_transitions:\n",
    "            agent.memory.push(*transition)\n",
    "\n",
    "        for _ in range(len(episode_transitions)):\n",
    "            agent.step()\n",
    "\n",
    "        rewards_per_episode.append(total_reward)\n",
    "        end_time = time.time()\n",
    "        episode_time = end_time - start_time\n",
    "        episode_times.append(episode_time)\n",
    "\n",
    "        if total_reward >= 20:\n",
    "            early_stop_counter += 1\n",
    "        else:\n",
    "            early_stop_counter = 0\n",
    "\n",
    "        if early_stop_counter >= 15:\n",
    "            print(f\"Early stopping at episode {episode}\")\n",
    "            torch.save(agent.policy_net.state_dict(), os.path.join(drive_path, \"dqn_pong_env_v5_early_stop_train.pth\"))\n",
    "            break\n",
    "\n",
    "        if episode % 10 == 0:\n",
    "            print(f\"Episode {episode + 1}: Total Reward: {total_reward}, Epsilon: {agent.epsilon:.3f}, Steps: {steps_taken}, Time: {episode_time:.2f}s\")\n",
    "            # Saving model\n",
    "            torch.save(agent.policy_net.state_dict(), os.path.join(drive_path, \"dqn_pong_env_v5.pth\"))\n",
    "            # Creating checkpoint dictionary and saving\n",
    "            checkpoint = {\n",
    "                'policy_net_state_dict': agent.policy_net.state_dict(),\n",
    "                'optimizer_state_dict': agent.optimizer.state_dict(),\n",
    "                'total_steps': agent.total_steps,\n",
    "                'current_episode': agent.current_episode,\n",
    "                'rewards_per_episode': rewards_per_episode,\n",
    "                'epsilon_values': epsilon_values,\n",
    "                'episode_times': episode_times\n",
    "            }\n",
    "            torch.save(checkpoint, os.path.join(drive_path, \"dqn_pong_checkpoints.pth\"))\n",
    "\n",
    "        if episode % target_update_interval == 0:\n",
    "            agent.update_target_network()\n",
    "\n",
    "    return agent, rewards_per_episode, epsilon_values, episode_times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xOCRUptB09NU"
   },
   "source": [
    "Training Pong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NMicpV5V1AUU",
    "outputId": "9c5ded76-f334-452a-a820-32eaee566653"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward: -20.0, Epsilon: 1.000, Steps: 837, Time: 6.55s\n",
      "Episode 11: Total Reward: -21.0, Epsilon: 0.977, Steps: 880, Time: 6.10s\n",
      "Episode 21: Total Reward: -21.0, Epsilon: 0.955, Steps: 869, Time: 6.04s\n",
      "Episode 31: Total Reward: -21.0, Epsilon: 0.933, Steps: 954, Time: 6.57s\n",
      "Episode 41: Total Reward: -19.0, Epsilon: 0.912, Steps: 1038, Time: 7.30s\n",
      "Episode 51: Total Reward: -20.0, Epsilon: 0.891, Steps: 933, Time: 6.55s\n",
      "Episode 61: Total Reward: -17.0, Epsilon: 0.871, Steps: 1252, Time: 8.67s\n",
      "Episode 71: Total Reward: -20.0, Epsilon: 0.851, Steps: 863, Time: 6.03s\n",
      "Episode 81: Total Reward: -21.0, Epsilon: 0.832, Steps: 876, Time: 6.09s\n",
      "Episode 91: Total Reward: -21.0, Epsilon: 0.813, Steps: 975, Time: 6.90s\n",
      "Episode 101: Total Reward: -18.0, Epsilon: 0.794, Steps: 1240, Time: 8.71s\n",
      "Episode 111: Total Reward: -20.0, Epsilon: 0.776, Steps: 898, Time: 6.36s\n",
      "Episode 121: Total Reward: -21.0, Epsilon: 0.759, Steps: 968, Time: 6.84s\n",
      "Episode 131: Total Reward: -21.0, Epsilon: 0.741, Steps: 817, Time: 5.78s\n",
      "Episode 141: Total Reward: -20.0, Epsilon: 0.724, Steps: 1108, Time: 7.85s\n",
      "Episode 151: Total Reward: -21.0, Epsilon: 0.708, Steps: 1110, Time: 7.89s\n",
      "Episode 161: Total Reward: -20.0, Epsilon: 0.692, Steps: 971, Time: 6.95s\n",
      "Episode 171: Total Reward: -18.0, Epsilon: 0.676, Steps: 1123, Time: 8.02s\n",
      "Episode 181: Total Reward: -14.0, Epsilon: 0.661, Steps: 1962, Time: 14.04s\n",
      "Episode 191: Total Reward: -18.0, Epsilon: 0.646, Steps: 1439, Time: 10.28s\n",
      "Episode 201: Total Reward: -19.0, Epsilon: 0.631, Steps: 1271, Time: 9.03s\n",
      "Episode 211: Total Reward: -19.0, Epsilon: 0.617, Steps: 1297, Time: 9.31s\n",
      "Episode 221: Total Reward: -20.0, Epsilon: 0.603, Steps: 1202, Time: 8.71s\n",
      "Episode 231: Total Reward: -21.0, Epsilon: 0.589, Steps: 1034, Time: 7.45s\n",
      "Episode 241: Total Reward: -16.0, Epsilon: 0.575, Steps: 1528, Time: 11.08s\n",
      "Episode 251: Total Reward: -18.0, Epsilon: 0.562, Steps: 1365, Time: 9.90s\n",
      "Episode 261: Total Reward: -19.0, Epsilon: 0.550, Steps: 1382, Time: 9.90s\n",
      "Episode 271: Total Reward: -19.0, Epsilon: 0.537, Steps: 1311, Time: 9.58s\n",
      "Episode 281: Total Reward: -19.0, Epsilon: 0.525, Steps: 1216, Time: 8.92s\n",
      "Episode 291: Total Reward: -20.0, Epsilon: 0.513, Steps: 1365, Time: 9.96s\n",
      "Episode 301: Total Reward: -16.0, Epsilon: 0.501, Steps: 1759, Time: 12.83s\n",
      "Episode 311: Total Reward: -16.0, Epsilon: 0.490, Steps: 1662, Time: 12.17s\n",
      "Episode 321: Total Reward: -19.0, Epsilon: 0.479, Steps: 1672, Time: 12.16s\n",
      "Episode 331: Total Reward: -14.0, Epsilon: 0.468, Steps: 1848, Time: 13.44s\n",
      "Episode 341: Total Reward: -12.0, Epsilon: 0.457, Steps: 2070, Time: 15.08s\n",
      "Episode 351: Total Reward: -20.0, Epsilon: 0.447, Steps: 1178, Time: 8.74s\n",
      "Episode 361: Total Reward: -17.0, Epsilon: 0.437, Steps: 1772, Time: 13.04s\n",
      "Episode 371: Total Reward: -17.0, Epsilon: 0.427, Steps: 1679, Time: 12.26s\n",
      "Episode 381: Total Reward: -12.0, Epsilon: 0.417, Steps: 2332, Time: 17.05s\n",
      "Episode 391: Total Reward: -13.0, Epsilon: 0.407, Steps: 2189, Time: 16.06s\n",
      "Episode 401: Total Reward: -13.0, Epsilon: 0.398, Steps: 1746, Time: 12.92s\n",
      "Episode 411: Total Reward: -19.0, Epsilon: 0.389, Steps: 1440, Time: 10.63s\n",
      "Episode 421: Total Reward: -14.0, Epsilon: 0.380, Steps: 2139, Time: 15.77s\n",
      "Episode 431: Total Reward: -15.0, Epsilon: 0.372, Steps: 1817, Time: 13.38s\n",
      "Episode 441: Total Reward: -13.0, Epsilon: 0.363, Steps: 2089, Time: 15.43s\n",
      "Episode 451: Total Reward: -12.0, Epsilon: 0.355, Steps: 2125, Time: 15.62s\n",
      "Episode 461: Total Reward: -13.0, Epsilon: 0.347, Steps: 1885, Time: 13.93s\n",
      "Episode 471: Total Reward: -13.0, Epsilon: 0.339, Steps: 2127, Time: 15.78s\n",
      "Episode 481: Total Reward: -13.0, Epsilon: 0.331, Steps: 1973, Time: 14.66s\n",
      "Episode 491: Total Reward: -9.0, Epsilon: 0.324, Steps: 2192, Time: 16.22s\n",
      "Episode 501: Total Reward: -15.0, Epsilon: 0.316, Steps: 1713, Time: 12.74s\n",
      "Episode 511: Total Reward: -15.0, Epsilon: 0.309, Steps: 2006, Time: 14.79s\n",
      "Episode 521: Total Reward: -10.0, Epsilon: 0.302, Steps: 2240, Time: 16.61s\n",
      "Episode 531: Total Reward: -10.0, Epsilon: 0.295, Steps: 2450, Time: 18.26s\n",
      "Episode 541: Total Reward: -14.0, Epsilon: 0.288, Steps: 1698, Time: 12.56s\n",
      "Episode 551: Total Reward: -14.0, Epsilon: 0.282, Steps: 1626, Time: 12.14s\n",
      "Episode 561: Total Reward: -16.0, Epsilon: 0.275, Steps: 1635, Time: 12.17s\n",
      "Episode 571: Total Reward: -11.0, Epsilon: 0.269, Steps: 2229, Time: 16.60s\n",
      "Episode 581: Total Reward: -13.0, Epsilon: 0.263, Steps: 2151, Time: 15.95s\n",
      "Episode 591: Total Reward: -15.0, Epsilon: 0.257, Steps: 1843, Time: 13.64s\n",
      "Episode 601: Total Reward: -17.0, Epsilon: 0.251, Steps: 1316, Time: 9.78s\n",
      "Episode 611: Total Reward: -10.0, Epsilon: 0.245, Steps: 1825, Time: 13.72s\n",
      "Episode 621: Total Reward: -15.0, Epsilon: 0.240, Steps: 1871, Time: 13.88s\n",
      "Episode 631: Total Reward: 2.0, Epsilon: 0.234, Steps: 3086, Time: 23.11s\n",
      "Episode 641: Total Reward: -11.0, Epsilon: 0.229, Steps: 1916, Time: 14.19s\n",
      "Episode 651: Total Reward: -10.0, Epsilon: 0.224, Steps: 1882, Time: 13.98s\n",
      "Episode 661: Total Reward: -14.0, Epsilon: 0.219, Steps: 1662, Time: 12.42s\n",
      "Episode 671: Total Reward: -13.0, Epsilon: 0.214, Steps: 1727, Time: 12.97s\n",
      "Episode 681: Total Reward: -9.0, Epsilon: 0.209, Steps: 2146, Time: 16.03s\n",
      "Episode 691: Total Reward: -10.0, Epsilon: 0.204, Steps: 2107, Time: 15.80s\n",
      "Episode 701: Total Reward: -13.0, Epsilon: 0.200, Steps: 1948, Time: 14.53s\n",
      "Episode 711: Total Reward: -15.0, Epsilon: 0.195, Steps: 1522, Time: 11.40s\n",
      "Episode 721: Total Reward: -12.0, Epsilon: 0.191, Steps: 1774, Time: 13.36s\n",
      "Episode 731: Total Reward: -10.0, Epsilon: 0.186, Steps: 2361, Time: 17.72s\n",
      "Episode 741: Total Reward: -16.0, Epsilon: 0.182, Steps: 1651, Time: 12.41s\n",
      "Episode 751: Total Reward: -12.0, Epsilon: 0.178, Steps: 1857, Time: 13.94s\n",
      "Episode 761: Total Reward: -5.0, Epsilon: 0.174, Steps: 2940, Time: 21.97s\n",
      "Episode 771: Total Reward: -16.0, Epsilon: 0.170, Steps: 1384, Time: 10.38s\n",
      "Episode 781: Total Reward: -19.0, Epsilon: 0.166, Steps: 1129, Time: 8.53s\n",
      "Episode 791: Total Reward: -9.0, Epsilon: 0.162, Steps: 2264, Time: 16.94s\n",
      "Episode 801: Total Reward: -15.0, Epsilon: 0.158, Steps: 1731, Time: 12.97s\n",
      "Episode 811: Total Reward: -9.0, Epsilon: 0.155, Steps: 2276, Time: 17.09s\n",
      "Episode 821: Total Reward: -5.0, Epsilon: 0.151, Steps: 2307, Time: 17.29s\n",
      "Episode 831: Total Reward: -6.0, Epsilon: 0.148, Steps: 2293, Time: 17.39s\n",
      "Episode 841: Total Reward: -13.0, Epsilon: 0.145, Steps: 1647, Time: 12.39s\n",
      "Episode 851: Total Reward: 2.0, Epsilon: 0.141, Steps: 2935, Time: 22.21s\n",
      "Episode 861: Total Reward: -6.0, Epsilon: 0.138, Steps: 2472, Time: 18.61s\n",
      "Episode 871: Total Reward: -13.0, Epsilon: 0.135, Steps: 1802, Time: 13.48s\n",
      "Episode 881: Total Reward: -6.0, Epsilon: 0.132, Steps: 2379, Time: 17.98s\n",
      "Episode 891: Total Reward: -12.0, Epsilon: 0.129, Steps: 1747, Time: 13.18s\n",
      "Episode 901: Total Reward: -8.0, Epsilon: 0.126, Steps: 2109, Time: 15.90s\n",
      "Episode 911: Total Reward: -10.0, Epsilon: 0.123, Steps: 1837, Time: 13.91s\n",
      "Episode 921: Total Reward: -13.0, Epsilon: 0.120, Steps: 1644, Time: 12.42s\n",
      "Episode 931: Total Reward: -10.0, Epsilon: 0.117, Steps: 1827, Time: 13.74s\n",
      "Episode 941: Total Reward: -8.0, Epsilon: 0.115, Steps: 1998, Time: 15.04s\n",
      "Episode 951: Total Reward: -10.0, Epsilon: 0.112, Steps: 1976, Time: 14.90s\n",
      "Episode 961: Total Reward: -6.0, Epsilon: 0.110, Steps: 2358, Time: 17.78s\n",
      "Episode 971: Total Reward: -8.0, Epsilon: 0.107, Steps: 2191, Time: 16.62s\n",
      "Episode 981: Total Reward: -10.0, Epsilon: 0.105, Steps: 1904, Time: 14.48s\n",
      "Episode 991: Total Reward: -11.0, Epsilon: 0.102, Steps: 1768, Time: 13.36s\n",
      "Episode 1001: Total Reward: -18.0, Epsilon: 0.100, Steps: 1029, Time: 7.78s\n",
      "Episode 1011: Total Reward: -12.0, Epsilon: 0.098, Steps: 1587, Time: 11.97s\n",
      "Episode 1021: Total Reward: -12.0, Epsilon: 0.095, Steps: 1664, Time: 12.53s\n",
      "Episode 1031: Total Reward: -1.0, Epsilon: 0.093, Steps: 2736, Time: 20.64s\n",
      "Episode 1041: Total Reward: -18.0, Epsilon: 0.091, Steps: 1453, Time: 11.07s\n",
      "Episode 1051: Total Reward: -8.0, Epsilon: 0.089, Steps: 2176, Time: 16.52s\n",
      "Episode 1061: Total Reward: -5.0, Epsilon: 0.087, Steps: 2217, Time: 16.74s\n",
      "Episode 1071: Total Reward: -19.0, Epsilon: 0.085, Steps: 934, Time: 7.08s\n",
      "Episode 1081: Total Reward: -9.0, Epsilon: 0.083, Steps: 1886, Time: 14.20s\n",
      "Episode 1091: Total Reward: 1.0, Epsilon: 0.081, Steps: 2729, Time: 20.69s\n",
      "Episode 1101: Total Reward: -1.0, Epsilon: 0.079, Steps: 2584, Time: 19.64s\n",
      "Episode 1111: Total Reward: -9.0, Epsilon: 0.078, Steps: 2057, Time: 15.64s\n",
      "Episode 1121: Total Reward: 4.0, Epsilon: 0.076, Steps: 2813, Time: 21.23s\n",
      "Episode 1131: Total Reward: -3.0, Epsilon: 0.074, Steps: 2904, Time: 22.04s\n",
      "Episode 1141: Total Reward: 2.0, Epsilon: 0.072, Steps: 3034, Time: 23.00s\n",
      "Episode 1151: Total Reward: -3.0, Epsilon: 0.071, Steps: 2609, Time: 19.78s\n",
      "Episode 1161: Total Reward: 1.0, Epsilon: 0.069, Steps: 3047, Time: 23.12s\n",
      "Episode 1171: Total Reward: 4.0, Epsilon: 0.068, Steps: 2550, Time: 19.42s\n",
      "Episode 1181: Total Reward: -1.0, Epsilon: 0.066, Steps: 2928, Time: 22.15s\n",
      "Episode 1191: Total Reward: 7.0, Epsilon: 0.065, Steps: 2427, Time: 18.37s\n",
      "Episode 1201: Total Reward: -3.0, Epsilon: 0.063, Steps: 2562, Time: 19.47s\n",
      "Episode 1211: Total Reward: -1.0, Epsilon: 0.062, Steps: 2460, Time: 18.74s\n",
      "Episode 1221: Total Reward: 6.0, Epsilon: 0.060, Steps: 2402, Time: 18.34s\n",
      "Episode 1231: Total Reward: 5.0, Epsilon: 0.059, Steps: 2524, Time: 19.12s\n",
      "Episode 1241: Total Reward: 8.0, Epsilon: 0.058, Steps: 2388, Time: 18.35s\n",
      "Episode 1251: Total Reward: 4.0, Epsilon: 0.056, Steps: 2321, Time: 17.58s\n",
      "Episode 1261: Total Reward: -18.0, Epsilon: 0.055, Steps: 997, Time: 7.57s\n",
      "Episode 1271: Total Reward: 16.0, Epsilon: 0.054, Steps: 1878, Time: 14.26s\n",
      "Episode 1281: Total Reward: -1.0, Epsilon: 0.052, Steps: 2699, Time: 20.56s\n",
      "Episode 1291: Total Reward: 11.0, Epsilon: 0.051, Steps: 2162, Time: 16.47s\n",
      "Episode 1301: Total Reward: -1.0, Epsilon: 0.050, Steps: 2800, Time: 21.32s\n",
      "Episode 1311: Total Reward: 15.0, Epsilon: 0.050, Steps: 2009, Time: 15.30s\n",
      "Episode 1321: Total Reward: 10.0, Epsilon: 0.050, Steps: 2242, Time: 16.96s\n",
      "Episode 1331: Total Reward: 10.0, Epsilon: 0.050, Steps: 2297, Time: 17.43s\n",
      "Episode 1341: Total Reward: -1.0, Epsilon: 0.050, Steps: 2744, Time: 20.73s\n",
      "Episode 1351: Total Reward: -1.0, Epsilon: 0.050, Steps: 2463, Time: 18.71s\n",
      "Episode 1361: Total Reward: 6.0, Epsilon: 0.050, Steps: 2551, Time: 19.52s\n",
      "Episode 1371: Total Reward: -5.0, Epsilon: 0.050, Steps: 2310, Time: 17.45s\n",
      "Episode 1381: Total Reward: 12.0, Epsilon: 0.050, Steps: 2204, Time: 16.86s\n",
      "Episode 1391: Total Reward: -1.0, Epsilon: 0.050, Steps: 2489, Time: 18.83s\n",
      "Episode 1401: Total Reward: -21.0, Epsilon: 0.050, Steps: 904, Time: 6.91s\n",
      "Episode 1411: Total Reward: -1.0, Epsilon: 0.050, Steps: 2388, Time: 18.13s\n",
      "Episode 1421: Total Reward: 12.0, Epsilon: 0.050, Steps: 2143, Time: 16.27s\n",
      "Episode 1431: Total Reward: 6.0, Epsilon: 0.050, Steps: 2443, Time: 18.60s\n",
      "Episode 1441: Total Reward: -1.0, Epsilon: 0.050, Steps: 2514, Time: 19.10s\n",
      "Episode 1451: Total Reward: -1.0, Epsilon: 0.050, Steps: 2616, Time: 19.79s\n",
      "Episode 1461: Total Reward: 18.0, Epsilon: 0.050, Steps: 1877, Time: 14.39s\n",
      "Episode 1471: Total Reward: -1.0, Epsilon: 0.050, Steps: 2979, Time: 22.70s\n",
      "Episode 1481: Total Reward: -1.0, Epsilon: 0.050, Steps: 3727, Time: 28.46s\n",
      "Episode 1491: Total Reward: -1.0, Epsilon: 0.050, Steps: 2563, Time: 19.56s\n",
      "Episode 1501: Total Reward: -1.0, Epsilon: 0.050, Steps: 2465, Time: 18.64s\n",
      "Episode 1511: Total Reward: 1.0, Epsilon: 0.050, Steps: 2677, Time: 20.38s\n",
      "Episode 1521: Total Reward: -1.0, Epsilon: 0.050, Steps: 2750, Time: 20.89s\n",
      "Episode 1531: Total Reward: -1.0, Epsilon: 0.050, Steps: 2878, Time: 21.86s\n",
      "Episode 1541: Total Reward: -1.0, Epsilon: 0.050, Steps: 2559, Time: 19.58s\n",
      "Episode 1551: Total Reward: -17.0, Epsilon: 0.050, Steps: 1112, Time: 8.41s\n",
      "Episode 1561: Total Reward: -1.0, Epsilon: 0.050, Steps: 2705, Time: 20.56s\n",
      "Episode 1571: Total Reward: -9.0, Epsilon: 0.050, Steps: 2014, Time: 15.19s\n",
      "Episode 1581: Total Reward: 17.0, Epsilon: 0.050, Steps: 1938, Time: 14.94s\n",
      "Episode 1591: Total Reward: -10.0, Epsilon: 0.050, Steps: 1936, Time: 14.69s\n",
      "Episode 1601: Total Reward: -1.0, Epsilon: 0.050, Steps: 2450, Time: 18.60s\n",
      "Episode 1611: Total Reward: 7.0, Epsilon: 0.050, Steps: 2456, Time: 18.56s\n",
      "Episode 1621: Total Reward: -1.0, Epsilon: 0.050, Steps: 2704, Time: 20.68s\n",
      "Episode 1631: Total Reward: -1.0, Epsilon: 0.050, Steps: 2431, Time: 18.56s\n",
      "Episode 1641: Total Reward: 9.0, Epsilon: 0.050, Steps: 2178, Time: 16.55s\n",
      "Episode 1651: Total Reward: 1.0, Epsilon: 0.050, Steps: 2527, Time: 19.15s\n",
      "Episode 1661: Total Reward: 1.0, Epsilon: 0.050, Steps: 2545, Time: 19.34s\n",
      "Episode 1671: Total Reward: -5.0, Epsilon: 0.050, Steps: 2370, Time: 17.98s\n",
      "Episode 1681: Total Reward: -1.0, Epsilon: 0.050, Steps: 2330, Time: 17.76s\n",
      "Episode 1691: Total Reward: 1.0, Epsilon: 0.050, Steps: 2609, Time: 19.90s\n",
      "Episode 1701: Total Reward: -1.0, Epsilon: 0.050, Steps: 2456, Time: 18.57s\n",
      "Episode 1711: Total Reward: -20.0, Epsilon: 0.050, Steps: 838, Time: 6.40s\n",
      "Episode 1721: Total Reward: -1.0, Epsilon: 0.050, Steps: 2497, Time: 19.05s\n",
      "Episode 1731: Total Reward: 9.0, Epsilon: 0.050, Steps: 2162, Time: 16.36s\n",
      "Episode 1741: Total Reward: -14.0, Epsilon: 0.050, Steps: 1552, Time: 11.76s\n",
      "Episode 1751: Total Reward: -1.0, Epsilon: 0.050, Steps: 2429, Time: 18.34s\n",
      "Episode 1761: Total Reward: -14.0, Epsilon: 0.050, Steps: 1404, Time: 10.81s\n",
      "Episode 1771: Total Reward: -19.0, Epsilon: 0.050, Steps: 916, Time: 6.93s\n",
      "Episode 1781: Total Reward: -1.0, Epsilon: 0.050, Steps: 2639, Time: 20.05s\n",
      "Episode 1791: Total Reward: -9.0, Epsilon: 0.050, Steps: 1826, Time: 13.88s\n",
      "Episode 1801: Total Reward: -1.0, Epsilon: 0.050, Steps: 2807, Time: 21.44s\n",
      "Episode 1811: Total Reward: -1.0, Epsilon: 0.050, Steps: 2599, Time: 19.71s\n",
      "Episode 1821: Total Reward: -14.0, Epsilon: 0.050, Steps: 1723, Time: 13.06s\n",
      "Episode 1831: Total Reward: -8.0, Epsilon: 0.050, Steps: 1892, Time: 14.38s\n",
      "Episode 1841: Total Reward: -2.0, Epsilon: 0.050, Steps: 2500, Time: 18.99s\n",
      "Episode 1851: Total Reward: -2.0, Epsilon: 0.050, Steps: 2824, Time: 21.50s\n",
      "Episode 1861: Total Reward: -6.0, Epsilon: 0.050, Steps: 2270, Time: 17.17s\n",
      "Episode 1871: Total Reward: 5.0, Epsilon: 0.050, Steps: 2277, Time: 17.27s\n",
      "Episode 1881: Total Reward: -4.0, Epsilon: 0.050, Steps: 2427, Time: 18.43s\n",
      "Episode 1891: Total Reward: 4.0, Epsilon: 0.050, Steps: 2447, Time: 18.68s\n",
      "Episode 1901: Total Reward: -1.0, Epsilon: 0.050, Steps: 2428, Time: 18.60s\n",
      "Episode 1911: Total Reward: -20.0, Epsilon: 0.050, Steps: 899, Time: 6.82s\n",
      "Episode 1921: Total Reward: -5.0, Epsilon: 0.050, Steps: 2072, Time: 15.77s\n",
      "Episode 1931: Total Reward: -2.0, Epsilon: 0.050, Steps: 2369, Time: 17.88s\n",
      "Episode 1941: Total Reward: 1.0, Epsilon: 0.050, Steps: 2390, Time: 18.18s\n",
      "Episode 1951: Total Reward: -14.0, Epsilon: 0.050, Steps: 1544, Time: 11.67s\n",
      "Episode 1961: Total Reward: 1.0, Epsilon: 0.050, Steps: 2616, Time: 19.98s\n",
      "Episode 1971: Total Reward: -1.0, Epsilon: 0.050, Steps: 2462, Time: 18.66s\n",
      "Episode 1981: Total Reward: -1.0, Epsilon: 0.050, Steps: 2502, Time: 19.08s\n",
      "Episode 1991: Total Reward: -1.0, Epsilon: 0.050, Steps: 2374, Time: 18.13s\n",
      "Episode 2001: Total Reward: -1.0, Epsilon: 0.050, Steps: 2488, Time: 19.02s\n",
      "Episode 2011: Total Reward: 1.0, Epsilon: 0.050, Steps: 2657, Time: 20.07s\n",
      "Episode 2021: Total Reward: -1.0, Epsilon: 0.050, Steps: 2728, Time: 20.88s\n",
      "Episode 2031: Total Reward: -1.0, Epsilon: 0.050, Steps: 2419, Time: 18.39s\n",
      "Episode 2041: Total Reward: -1.0, Epsilon: 0.050, Steps: 2532, Time: 19.31s\n",
      "Episode 2051: Total Reward: 1.0, Epsilon: 0.050, Steps: 2687, Time: 20.40s\n",
      "Episode 2061: Total Reward: 14.0, Epsilon: 0.050, Steps: 2010, Time: 15.36s\n",
      "Episode 2071: Total Reward: 2.0, Epsilon: 0.050, Steps: 2878, Time: 22.01s\n",
      "Episode 2081: Total Reward: 16.0, Epsilon: 0.050, Steps: 1881, Time: 14.33s\n",
      "Episode 2091: Total Reward: 7.0, Epsilon: 0.050, Steps: 2363, Time: 18.01s\n",
      "Episode 2101: Total Reward: 12.0, Epsilon: 0.050, Steps: 2199, Time: 16.82s\n",
      "Episode 2111: Total Reward: -1.0, Epsilon: 0.050, Steps: 2552, Time: 19.58s\n",
      "Episode 2121: Total Reward: 1.0, Epsilon: 0.050, Steps: 2493, Time: 19.03s\n",
      "Episode 2131: Total Reward: -14.0, Epsilon: 0.050, Steps: 1550, Time: 11.74s\n",
      "Episode 2141: Total Reward: -1.0, Epsilon: 0.050, Steps: 2521, Time: 19.13s\n",
      "Episode 2151: Total Reward: -2.0, Epsilon: 0.050, Steps: 2434, Time: 18.52s\n",
      "Episode 2161: Total Reward: -1.0, Epsilon: 0.050, Steps: 2493, Time: 19.15s\n",
      "Episode 2171: Total Reward: -1.0, Epsilon: 0.050, Steps: 2450, Time: 18.63s\n",
      "Episode 2181: Total Reward: -1.0, Epsilon: 0.050, Steps: 2487, Time: 19.07s\n",
      "Episode 2191: Total Reward: -12.0, Epsilon: 0.050, Steps: 1742, Time: 13.19s\n",
      "Episode 2201: Total Reward: -20.0, Epsilon: 0.050, Steps: 874, Time: 6.68s\n",
      "Episode 2211: Total Reward: -18.0, Epsilon: 0.050, Steps: 1170, Time: 8.90s\n",
      "Episode 2221: Total Reward: -10.0, Epsilon: 0.050, Steps: 1865, Time: 14.18s\n",
      "Episode 2231: Total Reward: -1.0, Epsilon: 0.050, Steps: 2495, Time: 19.08s\n",
      "Episode 2241: Total Reward: -1.0, Epsilon: 0.050, Steps: 2409, Time: 18.36s\n",
      "Episode 2251: Total Reward: -1.0, Epsilon: 0.050, Steps: 2505, Time: 18.93s\n",
      "Episode 2261: Total Reward: -1.0, Epsilon: 0.050, Steps: 2399, Time: 18.16s\n",
      "Episode 2271: Total Reward: 1.0, Epsilon: 0.050, Steps: 2753, Time: 20.96s\n",
      "Episode 2281: Total Reward: -2.0, Epsilon: 0.050, Steps: 2571, Time: 19.54s\n",
      "Episode 2291: Total Reward: -1.0, Epsilon: 0.050, Steps: 2606, Time: 19.76s\n",
      "Episode 2301: Total Reward: -1.0, Epsilon: 0.050, Steps: 2598, Time: 19.81s\n",
      "Episode 2311: Total Reward: 13.0, Epsilon: 0.050, Steps: 2018, Time: 15.32s\n",
      "Episode 2321: Total Reward: -1.0, Epsilon: 0.050, Steps: 2568, Time: 19.55s\n",
      "Episode 2331: Total Reward: 9.0, Epsilon: 0.050, Steps: 2344, Time: 17.99s\n",
      "Episode 2341: Total Reward: -1.0, Epsilon: 0.050, Steps: 2669, Time: 20.22s\n",
      "Episode 2351: Total Reward: 3.0, Epsilon: 0.050, Steps: 2578, Time: 19.63s\n",
      "Episode 2361: Total Reward: -1.0, Epsilon: 0.050, Steps: 2651, Time: 20.12s\n",
      "Episode 2371: Total Reward: -1.0, Epsilon: 0.050, Steps: 2501, Time: 18.88s\n",
      "Episode 2381: Total Reward: -1.0, Epsilon: 0.050, Steps: 2616, Time: 19.78s\n",
      "Episode 2391: Total Reward: -2.0, Epsilon: 0.050, Steps: 2292, Time: 17.52s\n",
      "Episode 2401: Total Reward: 5.0, Epsilon: 0.050, Steps: 2347, Time: 17.79s\n",
      "Episode 2411: Total Reward: -15.0, Epsilon: 0.050, Steps: 1249, Time: 9.59s\n",
      "Episode 2421: Total Reward: 5.0, Epsilon: 0.050, Steps: 2534, Time: 19.20s\n",
      "Episode 2431: Total Reward: 4.0, Epsilon: 0.050, Steps: 2554, Time: 19.34s\n",
      "Episode 2441: Total Reward: -1.0, Epsilon: 0.050, Steps: 2876, Time: 21.95s\n",
      "Episode 2451: Total Reward: -1.0, Epsilon: 0.050, Steps: 2536, Time: 19.18s\n",
      "Episode 2461: Total Reward: -1.0, Epsilon: 0.050, Steps: 2425, Time: 18.37s\n",
      "Episode 2471: Total Reward: -1.0, Epsilon: 0.050, Steps: 2490, Time: 19.07s\n",
      "Episode 2481: Total Reward: 1.0, Epsilon: 0.050, Steps: 2582, Time: 19.74s\n",
      "Episode 2491: Total Reward: -1.0, Epsilon: 0.050, Steps: 2452, Time: 18.57s\n",
      "Episode 2501: Total Reward: -11.0, Epsilon: 0.050, Steps: 1615, Time: 12.24s\n",
      "Episode 2511: Total Reward: -1.0, Epsilon: 0.050, Steps: 2501, Time: 19.00s\n",
      "Episode 2521: Total Reward: -16.0, Epsilon: 0.050, Steps: 1291, Time: 9.72s\n",
      "Episode 2531: Total Reward: -1.0, Epsilon: 0.050, Steps: 2570, Time: 19.52s\n",
      "Episode 2541: Total Reward: 1.0, Epsilon: 0.050, Steps: 2530, Time: 19.36s\n",
      "Episode 2551: Total Reward: 1.0, Epsilon: 0.050, Steps: 2434, Time: 18.43s\n",
      "Episode 2561: Total Reward: -1.0, Epsilon: 0.050, Steps: 2453, Time: 18.66s\n",
      "Episode 2571: Total Reward: -1.0, Epsilon: 0.050, Steps: 2399, Time: 18.05s\n",
      "Episode 2581: Total Reward: -4.0, Epsilon: 0.050, Steps: 2408, Time: 18.24s\n",
      "Episode 2591: Total Reward: -1.0, Epsilon: 0.050, Steps: 2811, Time: 21.49s\n",
      "Episode 2601: Total Reward: -1.0, Epsilon: 0.050, Steps: 2401, Time: 18.16s\n",
      "Episode 2611: Total Reward: -1.0, Epsilon: 0.050, Steps: 2591, Time: 19.80s\n",
      "Episode 2621: Total Reward: -1.0, Epsilon: 0.050, Steps: 2354, Time: 17.85s\n",
      "Episode 2631: Total Reward: -1.0, Epsilon: 0.050, Steps: 2470, Time: 18.81s\n",
      "Episode 2641: Total Reward: -4.0, Epsilon: 0.050, Steps: 2244, Time: 17.04s\n",
      "Episode 2651: Total Reward: -18.0, Epsilon: 0.050, Steps: 1222, Time: 9.35s\n",
      "Episode 2661: Total Reward: -1.0, Epsilon: 0.050, Steps: 2580, Time: 19.71s\n",
      "Episode 2671: Total Reward: -4.0, Epsilon: 0.050, Steps: 2533, Time: 19.18s\n",
      "Episode 2681: Total Reward: -14.0, Epsilon: 0.050, Steps: 1701, Time: 12.90s\n",
      "Episode 2691: Total Reward: -1.0, Epsilon: 0.050, Steps: 2412, Time: 18.32s\n",
      "Episode 2701: Total Reward: -5.0, Epsilon: 0.050, Steps: 2383, Time: 18.28s\n",
      "Episode 2711: Total Reward: -3.0, Epsilon: 0.050, Steps: 2525, Time: 19.25s\n",
      "Episode 2721: Total Reward: -21.0, Epsilon: 0.050, Steps: 762, Time: 5.80s\n",
      "Episode 2731: Total Reward: 1.0, Epsilon: 0.050, Steps: 2734, Time: 20.88s\n",
      "Episode 2741: Total Reward: -1.0, Epsilon: 0.050, Steps: 2528, Time: 19.11s\n",
      "Episode 2751: Total Reward: -1.0, Epsilon: 0.050, Steps: 2539, Time: 19.25s\n",
      "Episode 2761: Total Reward: 1.0, Epsilon: 0.050, Steps: 2429, Time: 18.53s\n",
      "Episode 2771: Total Reward: -1.0, Epsilon: 0.050, Steps: 2590, Time: 19.55s\n",
      "Episode 2781: Total Reward: -1.0, Epsilon: 0.050, Steps: 2724, Time: 20.52s\n",
      "Episode 2791: Total Reward: -2.0, Epsilon: 0.050, Steps: 2531, Time: 19.28s\n",
      "Episode 2801: Total Reward: -1.0, Epsilon: 0.050, Steps: 2546, Time: 19.41s\n",
      "Episode 2811: Total Reward: -1.0, Epsilon: 0.050, Steps: 2442, Time: 18.52s\n",
      "Episode 2821: Total Reward: -1.0, Epsilon: 0.050, Steps: 2392, Time: 18.47s\n",
      "Episode 2831: Total Reward: -1.0, Epsilon: 0.050, Steps: 2550, Time: 19.49s\n",
      "Episode 2841: Total Reward: -1.0, Epsilon: 0.050, Steps: 2445, Time: 18.62s\n",
      "Episode 2851: Total Reward: -7.0, Epsilon: 0.050, Steps: 2139, Time: 16.38s\n",
      "Episode 2861: Total Reward: -20.0, Epsilon: 0.050, Steps: 837, Time: 6.37s\n"
     ]
    }
   ],
   "source": [
    "env_name = \"PongNoFrameskip-v4\"\n",
    "pong_env = gym.make(env_name, render_mode=\"rgb_array\", frameskip=1)\n",
    "pong_env = AtariPreprocessing(\n",
    "    pong_env,\n",
    "    frame_skip=4,\n",
    "    grayscale_obs=True,\n",
    "    scale_obs=False,\n",
    "    terminal_on_life_loss=False\n",
    ")\n",
    "pong_env = FrameStackObservation(pong_env, stack_size=4)\n",
    "\n",
    "# Creating folder\n",
    "drive_path = '/content/drive/MyDrive/dqn_pong_checkpoints/'\n",
    "os.makedirs(drive_path, exist_ok=True)\n",
    "\n",
    "# Training\n",
    "trained_agent, rewards_per_episode, epsilon_values, episode_times = train_q_values(pong_env, env_name, training_episodes=5000)\n",
    "torch.save(trained_agent.policy_net.state_dict(), os.path.join(drive_path, \"dqn_pongnoframeskip_env_v5.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p1bLvmlY1iS1"
   },
   "outputs": [],
   "source": [
    "    # Relevant Plots\n",
    "    plt.plot(rewards_per_episode)\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.title(f\"{env_name} Reward Characteristics - Training\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(epsilon_values, label=f\"Decay rate = {DQNAgent.EPSILON_DECAY}\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Epsilon\")\n",
    "    plt.title(f\"{env_name} Epsilon Characteristics - Training\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(episode_times)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Time (seconds)\")\n",
    "    plt.title(f\"{env_name} Episode Processing Time\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f1WXGIQi4200"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
