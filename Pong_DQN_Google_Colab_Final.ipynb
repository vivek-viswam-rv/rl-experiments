{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Setup"
      ],
      "metadata": {
        "id": "XQDzpaYp0Bxm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install\n",
        "!pip install gymnasium[atari]\n",
        "!pip install autorom[accept-rom-license]\n",
        "!pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMoJce1azKI9",
        "outputId": "dd0b6ed7-98db-4763-c0ed-510d5ef998b8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium[atari] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[atari]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[atari]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[atari]) (4.13.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[atari]) (0.0.4)\n",
            "Requirement already satisfied: ale_py>=0.9 in /usr/local/lib/python3.11/dist-packages (from gymnasium[atari]) (0.10.2)\n",
            "Requirement already satisfied: autorom[accept-rom-license] in /usr/local/lib/python3.11/dist-packages (0.6.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from autorom[accept-rom-license]) (8.1.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from autorom[accept-rom-license]) (2.32.3)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.11/dist-packages (from autorom[accept-rom-license]) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->autorom[accept-rom-license]) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->autorom[accept-rom-license]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->autorom[accept-rom-license]) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->autorom[accept-rom-license]) (2025.1.31)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDvd5QT-zRo0",
        "outputId": "f7a17017-8f21-4453-d4c0-61acd44e1d9f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import gymnasium as gym\n",
        "from gymnasium.wrappers import AtariPreprocessing, FrameStackObservation\n",
        "import matplotlib.pyplot as plt\n",
        "import ale_py\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from collections import deque\n",
        "import time\n",
        "import os"
      ],
      "metadata": {
        "id": "WIwMct6azVVc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Register\n",
        "gym.register_envs(ale_py)"
      ],
      "metadata": {
        "id": "Zme46j01zXoE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining Classes"
      ],
      "metadata": {
        "id": "5uds8Ho50TFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DQN\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DQN, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(4, 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(64 * 7 * 7, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 6)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.fc(x)\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.buffer, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "# DQN Agent\n",
        "class DQNAgent:\n",
        "    BATCH_SIZE = 32\n",
        "    EPSILON_START = 1\n",
        "    EPSILON_CUTOFF = 0.05\n",
        "    EPSILON_DECAY = 0.9977 # Will take about 1000 episodes to reach 0.1 and 1300 to reach 0.05\n",
        "    GAMMA = 0.99\n",
        "    LR = 1e-4\n",
        "    MEMORY_SIZE = 30000\n",
        "    total_steps = 0\n",
        "\n",
        "    def __init__(self):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.policy_net = DQN().to(self.device)\n",
        "        self.target_net = DQN().to(self.device)\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "        self.target_net.eval()\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.LR)\n",
        "        self.memory = ReplayBuffer(self.MEMORY_SIZE)\n",
        "        self.steps_done = 0\n",
        "        self.epsilon = self.EPSILON_START\n",
        "        self.current_episode = 0\n",
        "\n",
        "    def select_e_greedy_action(self, env, state):\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randint(0, env.action_space.n - 1)\n",
        "        else:\n",
        "            state = torch.tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
        "            with torch.no_grad():\n",
        "                return self.policy_net(state).argmax(dim=1).item()\n",
        "\n",
        "    def select_greedy_action(self, state):\n",
        "        state = torch.tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            return self.policy_net(state).argmax(dim=1).item()\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        self.total_steps += 1\n",
        "        self.epsilon = max(self.EPSILON_CUTOFF, self.EPSILON_START * (self.EPSILON_DECAY ** self.current_episode))\n",
        "\n",
        "    def step(self):\n",
        "        if len(self.memory) < self.BATCH_SIZE:\n",
        "            return\n",
        "        batch = self.memory.sample(self.BATCH_SIZE)\n",
        "\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "        states = torch.tensor(np.array(states), dtype=torch.float32, device=self.device)\n",
        "        actions = torch.tensor(np.array(actions), dtype=torch.int64, device=self.device).unsqueeze(1)\n",
        "        rewards = torch.tensor(np.array(rewards), dtype=torch.float32, device=self.device).unsqueeze(1)\n",
        "        next_states = torch.tensor(np.array(next_states), dtype=torch.float32, device=self.device)\n",
        "        dones = torch.tensor(np.array(dones), dtype=torch.float32, device=self.device).unsqueeze(1)\n",
        "        current_q = self.policy_net(states).gather(1, actions)\n",
        "        next_q = self.target_net(next_states).max(1, keepdim=True)[0].detach()\n",
        "        target_q = rewards + (self.GAMMA * next_q * (1 - dones))\n",
        "\n",
        "        loss = F.mse_loss(target_q, current_q)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def update_target_network(self):\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())"
      ],
      "metadata": {
        "id": "4QJteQOd0OU0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Function"
      ],
      "metadata": {
        "id": "6XCd4I4k0at0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "u0DUDN-qzDw5"
      },
      "outputs": [],
      "source": [
        "def train_q_values(env, env_name, target_update_interval=10, training_episodes=1000, agent_class=DQNAgent, checkpoint_path=None):\n",
        "    agent = agent_class()\n",
        "    rewards_per_episode = []\n",
        "    epsilon_values = []\n",
        "    episode_times = []\n",
        "    start_episode = 0\n",
        "    early_stop_counter = 0\n",
        "\n",
        "    # Loading from last checkpoint in case training fails\n",
        "    drive_path = '/content/drive/MyDrive/dqn_pong_checkpoints/'\n",
        "    if checkpoint_path is not None:\n",
        "        checkpoint = torch.load(os.path.join(drive_path, checkpoint_path))\n",
        "        agent.policy_net.load_state_dict(checkpoint['policy_net_state_dict'])\n",
        "        agent.target_net.load_state_dict(checkpoint['policy_net_state_dict'])\n",
        "        agent.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        agent.total_steps = checkpoint['total_steps']\n",
        "        agent.current_episode = checkpoint['current_episode']\n",
        "        rewards_per_episode = checkpoint['rewards_per_episode']\n",
        "        epsilon_values = checkpoint['epsilon_values']\n",
        "        episode_times = checkpoint['episode_times']\n",
        "        start_episode = checkpoint['current_episode'] + 1\n",
        "        agent.epsilon = max(agent.EPSILON_CUTOFF, agent.EPSILON_START * (agent.EPSILON_DECAY ** agent.current_episode))\n",
        "        print(f\"Resumed training from episode {start_episode}\")\n",
        "\n",
        "    for episode in range(start_episode, training_episodes):\n",
        "        agent.current_episode = episode\n",
        "        start_time = time.time()\n",
        "        obs, _ = env.reset()\n",
        "        state = obs\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "        steps_taken = 0\n",
        "        episode_transitions = []\n",
        "\n",
        "        epsilon_values.append(agent.epsilon)\n",
        "\n",
        "        while not done:\n",
        "            action = agent.select_e_greedy_action(env, state)\n",
        "            obs, reward, done, _, _ = env.step(action)\n",
        "            next_state = np.array(obs, dtype=np.float32)\n",
        "            episode_transitions.append((state, action, reward, next_state, done))\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "            steps_taken += 1\n",
        "\n",
        "            agent.decay_epsilon()\n",
        "\n",
        "        for transition in episode_transitions:\n",
        "            agent.memory.push(*transition)\n",
        "\n",
        "        for _ in range(len(episode_transitions)):\n",
        "            agent.step()\n",
        "\n",
        "        rewards_per_episode.append(total_reward)\n",
        "        end_time = time.time()\n",
        "        episode_time = end_time - start_time\n",
        "        episode_times.append(episode_time)\n",
        "\n",
        "        if total_reward >= 20:\n",
        "            early_stop_counter += 1\n",
        "        else:\n",
        "            early_stop_counter = 0\n",
        "\n",
        "        if early_stop_counter >= 15:\n",
        "            print(f\"Early stopping at episode {episode}\")\n",
        "            torch.save(agent.policy_net.state_dict(), os.path.join(drive_path, \"dqn_pong_env_v5_early_stop_train.pth\"))\n",
        "            break\n",
        "\n",
        "        if episode % 10 == 0:\n",
        "            print(f\"Episode {episode + 1}: Total Reward: {total_reward}, Epsilon: {agent.epsilon:.3f}, Steps: {steps_taken}, Time: {episode_time:.2f}s\")\n",
        "            # Saving model\n",
        "            torch.save(agent.policy_net.state_dict(), os.path.join(drive_path, \"dqn_pong_env_v5.pth\"))\n",
        "            # Creating checkpoint dictionary and saving\n",
        "            checkpoint = {\n",
        "                'policy_net_state_dict': agent.policy_net.state_dict(),\n",
        "                'optimizer_state_dict': agent.optimizer.state_dict(),\n",
        "                'total_steps': agent.total_steps,\n",
        "                'current_episode': agent.current_episode,\n",
        "                'rewards_per_episode': rewards_per_episode,\n",
        "                'epsilon_values': epsilon_values,\n",
        "                'episode_times': episode_times\n",
        "            }\n",
        "            torch.save(checkpoint, os.path.join(drive_path, \"dqn_pong_checkpoints.pth\"))\n",
        "\n",
        "        if episode % target_update_interval == 0:\n",
        "            agent.update_target_network()\n",
        "\n",
        "    return agent, rewards_per_episode, epsilon_values, episode_times"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Pong"
      ],
      "metadata": {
        "id": "xOCRUptB09NU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env_name = \"PongNoFrameskip-v4\"\n",
        "pong_env = gym.make(env_name, render_mode=\"rgb_array\", frameskip=1)\n",
        "pong_env = AtariPreprocessing(\n",
        "    pong_env,\n",
        "    frame_skip=4,\n",
        "    grayscale_obs=True,\n",
        "    scale_obs=False,\n",
        "    terminal_on_life_loss=False\n",
        ")\n",
        "pong_env = FrameStackObservation(pong_env, stack_size=4)\n",
        "\n",
        "# Creating folder\n",
        "drive_path = '/content/drive/MyDrive/dqn_pong_checkpoints/'\n",
        "os.makedirs(drive_path, exist_ok=True)\n",
        "\n",
        "# Training\n",
        "trained_agent, rewards_per_episode, epsilon_values, episode_times = train_q_values(pong_env, env_name, training_episodes=5000)\n",
        "torch.save(trained_agent.policy_net.state_dict(), os.path.join(drive_path, \"dqn_pongnoframeskip_env_v5.pth\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMicpV5V1AUU",
        "outputId": "c62fb5d0-8b8d-4c74-87ee-c534d8b21ec3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1: Total Reward: -19.0, Epsilon: 1.000, Steps: 1298, Time: 9.78s\n",
            "Episode 11: Total Reward: -19.0, Epsilon: 0.977, Steps: 1032, Time: 7.62s\n",
            "Episode 21: Total Reward: -21.0, Epsilon: 0.955, Steps: 874, Time: 6.15s\n",
            "Episode 31: Total Reward: -21.0, Epsilon: 0.933, Steps: 847, Time: 6.10s\n",
            "Episode 41: Total Reward: -21.0, Epsilon: 0.912, Steps: 839, Time: 6.03s\n",
            "Episode 51: Total Reward: -19.0, Epsilon: 0.891, Steps: 1143, Time: 8.86s\n",
            "Episode 61: Total Reward: -21.0, Epsilon: 0.871, Steps: 761, Time: 5.26s\n",
            "Episode 71: Total Reward: -20.0, Epsilon: 0.851, Steps: 1062, Time: 7.37s\n",
            "Episode 81: Total Reward: -21.0, Epsilon: 0.832, Steps: 823, Time: 6.14s\n",
            "Episode 91: Total Reward: -21.0, Epsilon: 0.813, Steps: 849, Time: 6.18s\n",
            "Episode 101: Total Reward: -18.0, Epsilon: 0.794, Steps: 1230, Time: 9.33s\n",
            "Episode 111: Total Reward: -20.0, Epsilon: 0.776, Steps: 883, Time: 6.67s\n",
            "Episode 121: Total Reward: -21.0, Epsilon: 0.759, Steps: 920, Time: 7.11s\n",
            "Episode 131: Total Reward: -21.0, Epsilon: 0.741, Steps: 790, Time: 5.46s\n",
            "Episode 141: Total Reward: -18.0, Epsilon: 0.724, Steps: 1161, Time: 8.23s\n",
            "Episode 151: Total Reward: -21.0, Epsilon: 0.708, Steps: 902, Time: 6.53s\n",
            "Episode 161: Total Reward: -20.0, Epsilon: 0.692, Steps: 1098, Time: 8.13s\n",
            "Episode 171: Total Reward: -20.0, Epsilon: 0.676, Steps: 1051, Time: 7.29s\n",
            "Episode 181: Total Reward: -16.0, Epsilon: 0.661, Steps: 1378, Time: 10.15s\n",
            "Episode 191: Total Reward: -15.0, Epsilon: 0.646, Steps: 1697, Time: 12.43s\n",
            "Episode 201: Total Reward: -17.0, Epsilon: 0.631, Steps: 1293, Time: 9.73s\n",
            "Episode 211: Total Reward: -15.0, Epsilon: 0.617, Steps: 1728, Time: 12.80s\n",
            "Episode 221: Total Reward: -20.0, Epsilon: 0.603, Steps: 1161, Time: 9.03s\n",
            "Episode 231: Total Reward: -20.0, Epsilon: 0.589, Steps: 865, Time: 6.59s\n",
            "Episode 241: Total Reward: -19.0, Epsilon: 0.575, Steps: 1283, Time: 9.54s\n",
            "Episode 251: Total Reward: -19.0, Epsilon: 0.562, Steps: 999, Time: 7.89s\n",
            "Episode 261: Total Reward: -20.0, Epsilon: 0.550, Steps: 1137, Time: 8.02s\n",
            "Episode 271: Total Reward: -21.0, Epsilon: 0.537, Steps: 1214, Time: 9.12s\n",
            "Episode 281: Total Reward: -20.0, Epsilon: 0.525, Steps: 1111, Time: 8.15s\n",
            "Episode 291: Total Reward: -17.0, Epsilon: 0.513, Steps: 1538, Time: 11.55s\n",
            "Episode 301: Total Reward: -18.0, Epsilon: 0.501, Steps: 1616, Time: 12.14s\n",
            "Episode 311: Total Reward: -19.0, Epsilon: 0.490, Steps: 1206, Time: 8.34s\n",
            "Episode 321: Total Reward: -15.0, Epsilon: 0.479, Steps: 1871, Time: 14.04s\n",
            "Episode 331: Total Reward: -13.0, Epsilon: 0.468, Steps: 1810, Time: 14.14s\n",
            "Episode 341: Total Reward: -17.0, Epsilon: 0.457, Steps: 1355, Time: 10.10s\n",
            "Episode 351: Total Reward: -15.0, Epsilon: 0.447, Steps: 1877, Time: 13.89s\n",
            "Episode 361: Total Reward: -15.0, Epsilon: 0.437, Steps: 1777, Time: 13.63s\n",
            "Episode 371: Total Reward: -16.0, Epsilon: 0.427, Steps: 1714, Time: 12.65s\n",
            "Episode 381: Total Reward: -16.0, Epsilon: 0.417, Steps: 1506, Time: 11.71s\n",
            "Episode 391: Total Reward: -15.0, Epsilon: 0.407, Steps: 1505, Time: 11.52s\n",
            "Episode 401: Total Reward: -15.0, Epsilon: 0.398, Steps: 1976, Time: 14.51s\n",
            "Episode 411: Total Reward: -14.0, Epsilon: 0.389, Steps: 1865, Time: 13.71s\n",
            "Episode 421: Total Reward: -13.0, Epsilon: 0.380, Steps: 2021, Time: 14.96s\n",
            "Episode 431: Total Reward: -17.0, Epsilon: 0.372, Steps: 1267, Time: 9.34s\n",
            "Episode 441: Total Reward: -10.0, Epsilon: 0.363, Steps: 2104, Time: 15.33s\n",
            "Episode 451: Total Reward: -14.0, Epsilon: 0.355, Steps: 1653, Time: 12.19s\n",
            "Episode 461: Total Reward: -19.0, Epsilon: 0.347, Steps: 1417, Time: 10.50s\n",
            "Episode 471: Total Reward: -15.0, Epsilon: 0.339, Steps: 1775, Time: 13.63s\n",
            "Episode 481: Total Reward: -19.0, Epsilon: 0.331, Steps: 1554, Time: 11.55s\n",
            "Episode 491: Total Reward: -11.0, Epsilon: 0.324, Steps: 2469, Time: 18.05s\n",
            "Episode 501: Total Reward: -15.0, Epsilon: 0.316, Steps: 1795, Time: 13.70s\n",
            "Episode 511: Total Reward: -9.0, Epsilon: 0.309, Steps: 2275, Time: 16.58s\n",
            "Episode 521: Total Reward: -17.0, Epsilon: 0.302, Steps: 1373, Time: 10.72s\n",
            "Episode 531: Total Reward: -15.0, Epsilon: 0.295, Steps: 1802, Time: 13.60s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    # Relevant Plots\n",
        "    plt.plot(rewards_per_episode)\n",
        "    plt.xlabel(\"Iteration\")\n",
        "    plt.ylabel(\"Reward\")\n",
        "    plt.title(f\"{env_name} Reward Characteristics - Training\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(epsilon_values, label=f\"Decay rate = {DQNAgent.EPSILON_DECAY}\")\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Epsilon\")\n",
        "    plt.title(f\"{env_name} Epsilon Characteristics - Training\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(episode_times)\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Time (seconds)\")\n",
        "    plt.title(f\"{env_name} Episode Processing Time\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "p1bLvmlY1iS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f1WXGIQi4200"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}